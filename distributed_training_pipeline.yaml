# PIPELINE DEFINITION
# Name: dist-train
# Description: Skeleton pipeline with 4 stages sharing a PVC: dataset download, training, lm-eval, model registry
# Inputs:
#    shared_log_file: str [Default: 'pipeline_log.txt']
#    training_accelerate_full_state_at_epoch: bool
#    training_algorithm: str [Default: 'OSFT']
#    training_backend: str [Default: 'mini-trainer']
#    training_base_model: str [Default: 'Qwen/Qwen2.5-1.5B-Instruct']
#    training_checkpoint_at_epoch: bool [Default: False]
#    training_effective_batch_size: int [Default: 128.0]
#    training_envs: str [Default: '']
#    training_learning_rate: float [Default: 5e-06]
#    training_lr_scheduler: str [Default: 'cosine']
#    training_lr_scheduler_kwargs: str [Default: '']
#    training_lr_warmup_steps: int [Default: 0.0]
#    training_max_seq_len: int [Default: 8192.0]
#    training_max_tokens_per_gpu: int [Default: 64000.0]
#    training_metadata_annotations: str [Default: '']
#    training_metadata_labels: str [Default: '']
#    training_num_epochs: int [Default: 1.0]
#    training_resource_cpu_per_worker: str [Default: '8']
#    training_resource_gpu_per_worker: int [Default: 1.0]
#    training_resource_memory_per_worker: str [Default: '32Gi']
#    training_resource_num_procs_per_worker: int [Default: 1.0]
#    training_resource_num_workers: int [Default: 1.0]
#    training_resources_num_nodes: int [Default: 2.0]
#    training_save_final_checkpoint: bool [Default: True]
#    training_save_samples: int
#    training_seed: int [Default: 42.0]
#    training_target_patterns: str [Default: '']
#    training_unfreeze_rank_ratio: float [Default: 0.25]
#    training_unmask_messages: bool
#    training_use_liger: bool [Default: True]
#    training_use_processed_dataset: bool
components:
  comp-dataset-download:
    executorLabel: exec-dataset-download
    inputDefinitions:
      parameters:
        pvc_mount_path:
          description: Path where the shared PVC is mounted.
          parameterType: STRING
        shared_log_file:
          defaultValue: pipeline_log.txt
          description: Name of the shared log file.
          isOptional: true
          parameterType: STRING
    outputDefinitions:
      parameters:
        Output:
          parameterType: STRING
  comp-eval-lm-eval:
    executorLabel: exec-eval-lm-eval
    inputDefinitions:
      parameters:
        pvc_mount_path:
          description: Path where the shared PVC is mounted.
          parameterType: STRING
        shared_log_file:
          defaultValue: pipeline_log.txt
          description: Name of the shared log file.
          isOptional: true
          parameterType: STRING
    outputDefinitions:
      parameters:
        Output:
          parameterType: STRING
  comp-model-registry:
    executorLabel: exec-model-registry
    inputDefinitions:
      parameters:
        pvc_mount_path:
          description: Path where the shared PVC is mounted.
          parameterType: STRING
        shared_log_file:
          defaultValue: pipeline_log.txt
          description: Name of the shared log file.
          isOptional: true
          parameterType: STRING
    outputDefinitions:
      parameters:
        Output:
          parameterType: STRING
  comp-train-model:
    executorLabel: exec-train-model
    inputDefinitions:
      artifacts:
        dataset:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
          description: 'Input dataset artifact (preferred). If not present, this component

            will attempt to load from a remote path specified in dataset.metadata.

            - metadata["artifact_path"]: remote dataset path (e.g., s3://..., https://...,
            or HF repo id)

            - metadata["pvc_dir"]: pre-staged PVC directory to use if present'
          isOptional: true
      parameters:
        kubernetes_config:
          description: TaskConfig passthrough (volumes, mounts, env, resources, tolerations,
            etc.).
          isOptional: true
          parameterType: TASK_CONFIG
        pvc_path:
          description: Root of the workspace PVC for this run.
          parameterType: STRING
        training_accelerate_full_state_at_epoch:
          description: Whether to save full Accelerate state at each epoch (optional).
          isOptional: true
          parameterType: BOOLEAN
        training_algorithm:
          defaultValue: OSFT
          description: Training algorithm ("OSFT" | "SFT"). OSFT adds continual learning
            support.
          isOptional: true
          parameterType: STRING
        training_backend:
          defaultValue: mini-trainer
          description: Trainer backend variant (e.g., "mini-trainer").
          isOptional: true
          parameterType: STRING
        training_base_model:
          defaultValue: Qwen/Qwen2.5-1.5B-Instruct
          description: HuggingFace model ID to fine-tune (e.g., "Qwen/Qwen2.5-1.5B-Instruct").
          isOptional: true
          parameterType: STRING
        training_checkpoint_at_epoch:
          description: Save a checkpoint at each epoch boundary.
          isOptional: true
          parameterType: BOOLEAN
        training_data_output_dir:
          description: Optional secondary output directory on PVC.
          isOptional: true
          parameterType: STRING
        training_effective_batch_size:
          defaultValue: 128.0
          description: "Per-step batch size. Guidance:\n- 1 GPU: 16\u201332\n- 2 GPUs:\
            \ 32\u201364\n- 4 GPUs: 64\u2013128"
          isOptional: true
          parameterType: NUMBER_INTEGER
        training_envs:
          defaultValue: ''
          description: Comma-separated env overrides ("KEY=VAL,KEY=VAL").
          isOptional: true
          parameterType: STRING
        training_learning_rate:
          description: Learning rate (typ. 1e-6 to 1e-4; 5e-6 is a good OSFT default).
          isOptional: true
          parameterType: NUMBER_DOUBLE
        training_lr_scheduler:
          description: LR scheduler ("cosine" | "linear" | "constant").
          isOptional: true
          parameterType: STRING
        training_lr_scheduler_kwargs:
          defaultValue: ''
          description: 'Comma-delimited key=value string for scheduler kwargs

            (e.g., "num_cycles=1,num_warmup_steps=100").'
          isOptional: true
          parameterType: STRING
        training_lr_warmup_steps:
          description: LR warmup steps (0 for none).
          isOptional: true
          parameterType: NUMBER_INTEGER
        training_max_seq_len:
          defaultValue: 8192.0
          description: "Max sequence length (typical 2048\u20138192)."
          isOptional: true
          parameterType: NUMBER_INTEGER
        training_max_tokens_per_gpu:
          defaultValue: 64000.0
          description: Token budget per GPU for memory mgmt.
          isOptional: true
          parameterType: NUMBER_INTEGER
        training_metadata_annotations:
          defaultValue: ''
          description: Comma-separated annotations ("k=v,k=v") for pod template.
          isOptional: true
          parameterType: STRING
        training_metadata_labels:
          defaultValue: ''
          description: Comma-separated labels ("k=v,k=v") for pod template.
          isOptional: true
          parameterType: STRING
        training_num_epochs:
          description: "Number of epochs (1 = quick test; 3\u20135 = better convergence)."
          isOptional: true
          parameterType: NUMBER_INTEGER
        training_resource_cpu_per_worker:
          defaultValue: '8'
          description: CPU limit/request per worker (e.g., "8").
          isOptional: true
          parameterType: STRING
        training_resource_gpu_per_worker:
          defaultValue: 1.0
          description: GPUs per worker (e.g., 1). Typically equals num procs.
          isOptional: true
          parameterType: NUMBER_INTEGER
        training_resource_memory_per_worker:
          defaultValue: 32Gi
          description: Memory per worker (e.g., "32Gi").
          isOptional: true
          parameterType: STRING
        training_resource_num_procs_per_worker:
          defaultValue: 1.0
          description: Processes (ranks) per worker (usually equals GPUs/worker).
          isOptional: true
          parameterType: NUMBER_INTEGER
        training_resource_num_workers:
          defaultValue: 1.0
          description: Total worker pods (1 = single-node; 2+ = multi-node).
          isOptional: true
          parameterType: NUMBER_INTEGER
        training_save_final_checkpoint:
          description: Save the final model checkpoint.
          isOptional: true
          parameterType: BOOLEAN
        training_save_samples:
          description: Number of samples to save during SFT (optional).
          isOptional: true
          parameterType: NUMBER_INTEGER
        training_seed:
          description: Random seed for reproducibility.
          isOptional: true
          parameterType: NUMBER_INTEGER
        training_target_patterns:
          defaultValue: ''
          description: Comma-separated target modules/patterns (algorithm-specific).
          isOptional: true
          parameterType: STRING
        training_unfreeze_rank_ratio:
          defaultValue: 0.25
          isOptional: true
          parameterType: NUMBER_DOUBLE
        training_unmask_messages:
          description: Whether to unmask chat messages if applicable.
          isOptional: true
          parameterType: BOOLEAN
        training_use_liger:
          description: Enable Liger kernel optimizations (image must include kernels).
          isOptional: true
          parameterType: BOOLEAN
        training_use_processed_dataset:
          description: Whether dataset is already processed.
          isOptional: true
          parameterType: BOOLEAN
    outputDefinitions:
      artifacts:
        output_metrics:
          artifactType:
            schemaTitle: system.Metrics
            schemaVersion: 0.0.1
        output_model:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
      parameters:
        Output:
          parameterType: STRING
    taskConfigPassthroughs:
    - field: RESOURCES
    - field: KUBERNETES_TOLERATIONS
    - field: KUBERNETES_NODE_SELECTOR
    - field: KUBERNETES_AFFINITY
    - applyToTask: true
      field: ENV
    - applyToTask: true
      field: KUBERNETES_VOLUMES
deploymentSpec:
  executors:
    exec-dataset-download:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - dataset_download
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.15.2'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"' && \"\
          $0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef dataset_download(\n    pvc_mount_path: str,\n    shared_log_file:\
          \ str = \"pipeline_log.txt\",\n) -> str:\n    \"\"\"Download/prepare the\
          \ dataset.\n\n    This skeleton component writes a hello world message to\
          \ a shared file on the PVC.\n\n    Args:\n        pvc_mount_path: Path where\
          \ the shared PVC is mounted.\n        shared_log_file: Name of the shared\
          \ log file.\n\n    Returns:\n        Status message.\n    \"\"\"\n    import\
          \ os\n\n    message = \"Hello world from dataset download\"\n    print(message)\n\
          \n    # Write to shared file on PVC\n    log_path = os.path.join(pvc_mount_path,\
          \ shared_log_file)\n    with open(log_path, \"a\") as f:\n        f.write(message\
          \ + \"\\n\")\n\n    print(f\"Message written to {log_path}\")\n    return\
          \ \"dataset_download completed\"\n\n"
        image: quay.io/opendatahub/odh-training-th03-cuda128-torch28-py312-rhel9@sha256:84d05c5ef9dd3c6ff8173c93dca7e2e6a1cab290f416fb2c469574f89b8e6438
    exec-eval-lm-eval:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - eval_lm_eval
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.15.2'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"' && \"\
          $0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef eval_lm_eval(\n    pvc_mount_path: str,\n    shared_log_file:\
          \ str = \"pipeline_log.txt\",\n) -> str:\n    \"\"\"Evaluate model using\
          \ lm-eval.\n\n    This skeleton component writes a hello world message to\
          \ a shared file on the PVC.\n\n    Args:\n        pvc_mount_path: Path where\
          \ the shared PVC is mounted.\n        shared_log_file: Name of the shared\
          \ log file.\n\n    Returns:\n        Status message.\n    \"\"\"\n    import\
          \ os\n\n    message = \"Hello world from eval with lm-eval\"\n    print(message)\n\
          \n    # Write to shared file on PVC\n    log_path = os.path.join(pvc_mount_path,\
          \ shared_log_file)\n    with open(log_path, \"a\") as f:\n        f.write(message\
          \ + \"\\n\")\n\n    print(f\"Message written to {log_path}\")\n    return\
          \ \"eval_lm_eval completed\"\n\n"
        image: quay.io/opendatahub/odh-training-th03-cuda128-torch28-py312-rhel9@sha256:84d05c5ef9dd3c6ff8173c93dca7e2e6a1cab290f416fb2c469574f89b8e6438
    exec-model-registry:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - model_registry
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.15.2'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"' && \"\
          $0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef model_registry(\n    pvc_mount_path: str,\n    shared_log_file:\
          \ str = \"pipeline_log.txt\",\n) -> str:\n    \"\"\"Register model to the\
          \ model registry.\n\n    This skeleton component writes a hello world message\
          \ to a shared file on the PVC,\n    then reads and prints the entire log\
          \ file to verify all 4 stages completed.\n\n    Args:\n        pvc_mount_path:\
          \ Path where the shared PVC is mounted.\n        shared_log_file: Name of\
          \ the shared log file.\n\n    Returns:\n        Contents of the shared log\
          \ file.\n    \"\"\"\n    import os\n\n    message = \"Hello world from model\
          \ registry\"\n    print(message)\n\n    # Write to shared file on PVC\n\
          \    log_path = os.path.join(pvc_mount_path, shared_log_file)\n    with\
          \ open(log_path, \"a\") as f:\n        f.write(message + \"\\n\")\n\n  \
          \  print(f\"Message written to {log_path}\")\n\n    # Read and print the\
          \ entire log file\n    print(\"\\n\" + \"=\" * 50)\n    print(\"PIPELINE\
          \ EXECUTION LOG:\")\n    print(\"=\" * 50)\n\n    with open(log_path, \"\
          r\") as f:\n        contents = f.read()\n\n    print(contents)\n    print(\"\
          =\" * 50)\n\n    # Verify we have 4 hello worlds\n    lines = [line for\
          \ line in contents.strip().split(\"\\n\") if line.startswith(\"Hello world\"\
          )]\n    print(f\"\\nTotal hello world messages: {len(lines)}\")\n\n    if\
          \ len(lines) == 4:\n        print(\"[OK] All 4 pipeline stages completed\
          \ successfully!\")\n    else:\n        print(f\"[FAIL] Expected 4 stages,\
          \ but found {len(lines)}\")\n\n    return contents\n\n"
        image: quay.io/opendatahub/odh-training-th03-cuda128-torch28-py312-rhel9@sha256:84d05c5ef9dd3c6ff8173c93dca7e2e6a1cab290f416fb2c469574f89b8e6438
    exec-train-model:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - train_model
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kubernetes'\
          \  &&  python3 -m pip install --quiet --no-warn-script-location 'kfp==2.15.2'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"' && \"\
          $0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef train_model(\n    # Workspace/PVC root (pass dsl.WORKSPACE_PATH_PLACEHOLDER\
          \ at call site)\n    pvc_path: str,\n    # Outputs (no defaults)\n    output_model:\
          \ dsl.Output[dsl.Model],\n    output_metrics: dsl.Output[dsl.Metrics],\n\
          \    # Dataset input and optional remote artifact path via metadata (e.g.,\
          \ s3://...)\n    dataset: dsl.Input[dsl.Dataset] = None,\n    # Base model\
          \ (HF ID or local path)\n    training_base_model: str = \"Qwen/Qwen2.5-1.5B-Instruct\"\
          ,\n    # Training algorithm selector\n    training_algorithm: str = \"OSFT\"\
          ,\n    # OSFT parameters (prefixed with training_)\n    training_unfreeze_rank_ratio:\
          \ float = 0.25,\n    training_effective_batch_size: int = 128,\n    training_max_tokens_per_gpu:\
          \ int = 64000,\n    training_max_seq_len: int = 8192,\n    training_learning_rate:\
          \ Optional[float] = None,\n    training_backend: str = \"mini-trainer\"\
          ,\n    training_target_patterns: str = \"\",\n    training_seed: Optional[int]\
          \ = None,\n    training_use_liger: Optional[bool] = None,\n    training_use_processed_dataset:\
          \ Optional[bool] = None,\n    training_unmask_messages: Optional[bool] =\
          \ None,\n    training_lr_scheduler: Optional[str] = None,\n    training_lr_warmup_steps:\
          \ Optional[int] = None,\n    training_save_samples: Optional[int] = None,\n\
          \    training_accelerate_full_state_at_epoch: Optional[bool] = None,\n \
          \   training_lr_scheduler_kwargs: str = \"\",\n    training_checkpoint_at_epoch:\
          \ Optional[bool] = None,\n    training_save_final_checkpoint: Optional[bool]\
          \ = None,\n    training_num_epochs: Optional[int] = None,\n    training_data_output_dir:\
          \ Optional[str] = None,\n    # Env overrides: \"KEY=VAL,KEY=VAL\"\n    training_envs:\
          \ str = \"\",\n    # Resource and runtime parameters (per worker/pod)\n\
          \    training_resource_cpu_per_worker: str = \"8\",\n    training_resource_gpu_per_worker:\
          \ int = 1,\n    training_resource_memory_per_worker: str = \"32Gi\",\n \
          \   training_resource_num_procs_per_worker: int = 1,\n    training_resource_num_workers:\
          \ int = 1,\n    training_metadata_labels: str = \"\",\n    training_metadata_annotations:\
          \ str = \"\",\n    # KFP TaskConfig passthrough for volumes/env/resources,\
          \ etc.\n    kubernetes_config: dsl.TaskConfig = None,\n) -> str:\n    \"\
          \"\"Perform model training (inline) using PVC workspace and TrainingHub\
          \ runtime.\n\n    Args:\n        pvc_path: Root of the workspace PVC for\
          \ this run.\n        dataset: Input dataset artifact (preferred). If not\
          \ present, this component\n            will attempt to load from a remote\
          \ path specified in dataset.metadata.\n            - metadata[\"artifact_path\"\
          ]: remote dataset path (e.g., s3://..., https://..., or HF repo id)\n  \
          \          - metadata[\"pvc_dir\"]: pre-staged PVC directory to use if present\n\
          \        training_base_model: HuggingFace model ID to fine-tune (e.g., \"\
          Qwen/Qwen2.5-1.5B-Instruct\").\n\n        training_algorithm: Training algorithm\
          \ (\"OSFT\" | \"SFT\"). OSFT adds continual learning support.\n        training_effective_batch_size:\
          \ Per-step batch size. Guidance:\n            - 1 GPU: 16\u201332\n    \
          \        - 2 GPUs: 32\u201364\n            - 4 GPUs: 64\u2013128\n     \
          \   training_max_tokens_per_gpu: Token budget per GPU for memory mgmt.\n\
          \        training_max_seq_len: Max sequence length (typical 2048\u20138192).\n\
          \        training_learning_rate: Learning rate (typ. 1e-6 to 1e-4; 5e-6\
          \ is a good OSFT default).\n        training_backend: Trainer backend variant\
          \ (e.g., \"mini-trainer\").\n        training_target_patterns: Comma-separated\
          \ target modules/patterns (algorithm-specific).\n        training_seed:\
          \ Random seed for reproducibility.\n        training_use_liger: Enable Liger\
          \ kernel optimizations (image must include kernels).\n        training_use_processed_dataset:\
          \ Whether dataset is already processed.\n        training_unmask_messages:\
          \ Whether to unmask chat messages if applicable.\n        training_lr_scheduler:\
          \ LR scheduler (\"cosine\" | \"linear\" | \"constant\").\n        training_lr_warmup_steps:\
          \ LR warmup steps (0 for none).\n        training_save_samples: Number of\
          \ samples to save during SFT (optional).\n        training_accelerate_full_state_at_epoch:\
          \ Whether to save full Accelerate state at each epoch (optional).\n    \
          \    training_lr_scheduler_kwargs: Comma-delimited key=value string for\
          \ scheduler kwargs\n            (e.g., \"num_cycles=1,num_warmup_steps=100\"\
          ).\n        training_checkpoint_at_epoch: Save a checkpoint at each epoch\
          \ boundary.\n        training_save_final_checkpoint: Save the final model\
          \ checkpoint.\n        training_num_epochs: Number of epochs (1 = quick\
          \ test; 3\u20135 = better convergence).\n        training_data_output_dir:\
          \ Optional secondary output directory on PVC.\n\n        training_envs:\
          \ Comma-separated env overrides (\"KEY=VAL,KEY=VAL\").\n        training_resource_cpu_per_worker:\
          \ CPU limit/request per worker (e.g., \"8\").\n        training_resource_gpu_per_worker:\
          \ GPUs per worker (e.g., 1). Typically equals num procs.\n        training_resource_memory_per_worker:\
          \ Memory per worker (e.g., \"32Gi\").\n        training_resource_num_procs_per_worker:\
          \ Processes (ranks) per worker (usually equals GPUs/worker).\n        training_resource_num_workers:\
          \ Total worker pods (1 = single-node; 2+ = multi-node).\n        training_metadata_labels:\
          \ Comma-separated labels (\"k=v,k=v\") for pod template.\n        training_metadata_annotations:\
          \ Comma-separated annotations (\"k=v,k=v\") for pod template.\n        kubernetes_config:\
          \ TaskConfig passthrough (volumes, mounts, env, resources, tolerations,\
          \ etc.).\n\n        output_model: Final model artifact copied to artifact\
          \ store and PVC,\n            with metadata set for downstream consumers:\n\
          \            - model_name: the fine-tuned base model id\n            - artifact_path:\
          \ output_model.path (artifact store path)\n            - pvc_model_dir:\
          \ \"<pvc_path>/final_model\" (PVC directory path)\n        output_metrics:\
          \ Logged numeric metrics (floats), e.g.:\n            - num_epochs, effective_batch_size,\
          \ learning_rate, max_seq_len\n            - max_tokens_per_gpu, unfreeze_rank_ratio\
          \ (0 for SFT)\n\n    OSFT func_args schema (passed to the trainer):\n\n\
          \        model_path: Path to the model to fine-tune\n\n        data_path:\
          \ Path to the training data\n\n        ckpt_output_dir: Directory to save\
          \ checkpoints\n\n        backend: Backend implementation to use (default:\
          \ \"instructlab-training\")\n\n        num_epochs: Number of training epochs\n\
          \n        effective_batch_size: Effective batch size for training\n\n  \
          \      learning_rate: Learning rate for training\n\n        max_seq_len:\
          \ Maximum sequence length\n\n        max_tokens_per_gpu: Maximum tokens\
          \ per GPU in a mini-batch (hard-cap for memory to avoid OOMs). Used to automatically\
          \ calculate mini-batch size and gradient accumulation to maintain the desired\
          \ effective_batch_size while staying within memory limits.\n\n        data_output_dir:\
          \ Directory to save processed data\n\n        save_samples: Number of samples\
          \ to save after training (0 disables saving based on sample count)\n\n \
          \       warmup_steps: Number of warmup steps\n\n        accelerate_full_state_at_epoch:\
          \ Whether to save full state at epoch for automatic checkpoint resumption\n\
          \n        checkpoint_at_epoch: Whether to checkpoint at each epoch\n\n \
          \   Returns:\n        Status message string.\n    \"\"\"\n    import os,\
          \ sys, json, time, logging, re\n    from typing import Dict, List, Tuple,\
          \ Optional as _Optional\n\n    # ------------------------------\n    # Logging\
          \ configuration\n    # ------------------------------\n    def _setup_logger()\
          \ -> logging.Logger:\n        \"\"\"Configure and return a logger for this\
          \ component.\"\"\"\n        _logger = logging.getLogger(\"train_model\"\
          )\n        _logger.setLevel(logging.INFO)\n        if not _logger.handlers:\n\
          \            _ch = logging.StreamHandler(sys.stdout)\n            _ch.setLevel(logging.INFO)\n\
          \            _ch.setFormatter(logging.Formatter(\"%(asctime)s - %(levelname)s\
          \ - %(message)s\"))\n            _logger.addHandler(_ch)\n        return\
          \ _logger\n\n    logger = _setup_logger()\n    logger.info(\"Initializing\
          \ training component\")\n    logger.info(f\"pvc_path={pvc_path}, model_name={training_base_model}\"\
          )\n\n    # ------------------------------\n    # Utility: latest checkpoint\n\
          \    # ------------------------------\n    def find_most_recent_checkpoint(checkpoints_root:\
          \ str) -> _Optional[str]:\n        if not os.path.isdir(checkpoints_root):\n\
          \            return None\n        latest: _Optional[Tuple[float, str]] =\
          \ None\n        for entry in os.listdir(checkpoints_root):\n           \
          \ full = os.path.join(checkpoints_root, entry)\n            if os.path.isdir(full):\n\
          \                try:\n                    mtime = os.path.getmtime(full)\n\
          \                except OSError:\n                    continue\n       \
          \         if latest is None or mtime > latest[0]:\n                    latest\
          \ = (mtime, full)\n        return latest[1] if latest else None\n\n    #\
          \ ------------------------------\n    # Kubernetes connection\n    # ------------------------------\n\
          \    def _init_k8s_client() -> _Optional[\"k8s_client.ApiClient\"]:\n  \
          \      \"\"\"Initialize and return a Kubernetes client from env (server/token)\
          \ or in-cluster/kubeconfig.\"\"\"\n        try:\n            from kubernetes\
          \ import client as k8s_client, config as k8s_config\n            env_server\
          \ = os.environ.get(\"KUBERNETES_SERVER_URL\", \"\").strip()\n          \
          \  env_token = os.environ.get(\"KUBERNETES_AUTH_TOKEN\", \"\").strip()\n\
          \            if env_server and env_token:\n                logger.info(\"\
          Configuring Kubernetes client from env (KUBERNETES_SERVER_URL/_AUTH_TOKEN)\"\
          )\n                cfg = k8s_client.Configuration()\n                cfg.host\
          \ = env_server\n                cfg.verify_ssl = False\n               \
          \ cfg.api_key = {\"authorization\": f\"Bearer {env_token}\"}\n         \
          \       k8s_client.Configuration.set_default(cfg)\n                return\
          \ k8s_client.ApiClient(cfg)\n            logger.info(\"Configuring Kubernetes\
          \ client in-cluster (or local kubeconfig)\")\n            try:\n       \
          \         k8s_config.load_incluster_config()\n            except Exception:\n\
          \                k8s_config.load_kube_config()\n            return k8s_client.ApiClient()\n\
          \        except Exception as _exc:\n            logger.warning(f\"Kubernetes\
          \ client not initialized: {_exc}\")\n            return None\n\n    _api_client\
          \ = _init_k8s_client()\n\n    # ------------------------------\n    # Environment\
          \ variables (defaults + overrides)\n    # ------------------------------\n\
          \    cache_root = os.path.join(pvc_path, \".cache\", \"huggingface\")\n\
          \    default_env: Dict[str, str] = {\n        \"XDG_CACHE_HOME\": \"/tmp\"\
          ,\n        \"TRITON_CACHE_DIR\": \"/tmp/.triton\",\n        \"HF_HOME\"\
          : \"/tmp/.cache/huggingface\",\n        \"HF_DATASETS_CACHE\": os.path.join(cache_root,\
          \ \"datasets\"),\n        \"TRANSFORMERS_CACHE\": os.path.join(cache_root,\
          \ \"transformers\"),\n        \"NCCL_DEBUG\": \"INFO\",\n    }\n\n    def\
          \ parse_kv_list(kv_str: str) -> Dict[str, str]:\n        out: Dict[str,\
          \ str] = {}\n        if not kv_str:\n            return out\n        for\
          \ item in kv_str.split(\",\"):\n            item = item.strip()\n      \
          \      if not item:\n                continue\n            if \"=\" not\
          \ in item:\n                raise ValueError(f\"Invalid key=value item (expected\
          \ key=value): {item}\")\n            k, v = item.split(\"=\", 1)\n     \
          \       k = k.strip()\n            v = v.strip()\n            if not k:\n\
          \                raise ValueError(f\"Invalid key in key=value pair: {item}\"\
          )\n            out[k] = v\n        return out\n\n    def _configure_env(env_csv:\
          \ str, base_env: Dict[str, str]) -> Dict[str, str]:\n        \"\"\"Merge\
          \ base env with CSV overrides and export them to process env; return merged\
          \ map.\"\"\"\n        overrides = parse_kv_list(env_csv)\n        merged\
          \ = {**base_env, **overrides}\n        for ek, ev in merged.items():\n \
          \           os.environ[ek] = ev\n        logger.info(f\"Env configured (keys):\
          \ {sorted(list(merged.keys()))}\")\n        return merged\n\n    merged_env\
          \ = _configure_env(training_envs, default_env)\n\n    # ------------------------------\n\
          \    # Dataset resolution\n    # ------------------------------\n    from\
          \ datasets import load_dataset, load_from_disk, Dataset\n    import shutil\n\
          \n    resolved_dataset_dir = os.path.join(pvc_path, \"dataset\", \"train\"\
          )\n    os.makedirs(resolved_dataset_dir, exist_ok=True)\n\n    def is_local_path(p:\
          \ str) -> bool:\n        return bool(p) and os.path.exists(p)\n\n    def\
          \ looks_like_url(p: str) -> bool:\n        return p.startswith(\"s3://\"\
          ) or p.startswith(\"http://\") or p.startswith(\"https://\")\n\n    def\
          \ _resolve_dataset(input_dataset: _Optional[dsl.Input[dsl.Dataset]], out_dir:\
          \ str) -> None:\n        \"\"\"Resolve dataset with preference: existing\
          \ PVC dir > input artifact > remote artifact/HF > default.\n        Remote\
          \ path is read from input_dataset.metadata['artifact_path'] if present.\
          \ If metadata['pvc_dir'] exists, prefer it.\n        \"\"\"\n        # 0)\
          \ If already present (e.g., staged by prior step), keep it\n        if os.path.isdir(out_dir)\
          \ and any(os.scandir(out_dir)):\n            logger.info(f\"Using existing\
          \ dataset at {out_dir}\")\n            return\n        # 1) Input artifact\n\
          \        if input_dataset and getattr(input_dataset, \"path\", None) and\
          \ os.path.exists(input_dataset.path):\n            logger.info(f\"Copying\
          \ input dataset from {input_dataset.path} to {out_dir}\")\n            shutil.copytree(input_dataset.path,\
          \ out_dir, dirs_exist_ok=True)\n            return\n        # 2) Remote\
          \ artifact (S3/HTTP) or HF repo id\n        rp = \"\"\n        try:\n  \
          \          if input_dataset and hasattr(input_dataset, \"metadata\") and\
          \ isinstance(input_dataset.metadata, dict):\n                pvc_dir = (input_dataset.metadata.get(\"\
          pvc_dir\") or \"\").strip()\n                if pvc_dir and os.path.isdir(pvc_dir)\
          \ and any(os.scandir(pvc_dir)):\n                    logger.info(f\"Using\
          \ pre-staged PVC dataset at {pvc_dir}\")\n                    shutil.copytree(pvc_dir,\
          \ out_dir, dirs_exist_ok=True)\n                    return\n           \
          \     rp = (input_dataset.metadata.get(\"artifact_path\") or \"\").strip()\n\
          \        except Exception:\n            rp = \"\"\n        if rp:\n    \
          \        if looks_like_url(rp):\n                logger.info(f\"Attempting\
          \ to load remote dataset from {rp}\")\n                # Try a few common\
          \ formats via datasets library\n                ext = rp.lower()\n     \
          \           try:\n                    if ext.endswith(\".json\") or ext.endswith(\"\
          .jsonl\"):\n                        ds: Dataset = load_dataset(\"json\"\
          , data_files=rp, split=\"train\")\n                    elif ext.endswith(\"\
          .parquet\"):\n                        ds: Dataset = load_dataset(\"parquet\"\
          , data_files=rp, split=\"train\")\n                    else:\n         \
          \               raise ValueError(\n                            \"Unsupported\
          \ remote dataset format. Provide a JSON/JSONL/PARQUET file or a HF dataset\
          \ repo id.\"\n                        )\n                    ds.save_to_disk(out_dir)\n\
          \                    return\n                except Exception as e:\n  \
          \                  raise ValueError(f\"Failed to load remote dataset from\
          \ {rp}: {e}\")\n            else:\n                # Treat as HF dataset\
          \ repo id\n                logger.info(f\"Assuming HF dataset repo id: {rp}\"\
          )\n                ds: Dataset = load_dataset(rp, split=\"train\")\n   \
          \             ds.save_to_disk(out_dir)\n                return\n       \
          \ # 3) Default fallback (Table-GPT)\n        logger.info(\"No dataset provided.\
          \ Falling back to 'LipengCS/Table-GPT'\")\n        ds: Dataset = load_dataset(\"\
          LipengCS/Table-GPT\", \"All\", split=\"train\")\n        ds.save_to_disk(out_dir)\n\
          \n    _resolve_dataset(dataset, resolved_dataset_dir)\n\n    # Export dataset\
          \ to JSONL so downstream trainer reads a plain JSONL file\n    jsonl_path\
          \ = os.path.join(resolved_dataset_dir, \"train.jsonl\")\n    try:\n    \
          \    # Try loading from the saved HF dataset on disk and export to JSONL\n\
          \        ds_on_disk = load_from_disk(resolved_dataset_dir)\n        # Handle\
          \ DatasetDict vs Dataset\n        train_split = ds_on_disk[\"train\"] if\
          \ isinstance(ds_on_disk, dict) else ds_on_disk\n        try:\n         \
          \   # Newer datasets supports native JSON export\n            train_split.to_json(jsonl_path,\
          \ lines=True)\n            logger.info(f\"Wrote JSONL to {jsonl_path} via\
          \ to_json\")\n        except AttributeError:\n            # Manual JSONL\
          \ write\n            import json as _json\n            with open(jsonl_path,\
          \ \"w\") as _f:\n                for _rec in train_split:\n            \
          \        _f.write(_json.dumps(_rec, ensure_ascii=False) + \"\\n\")\n   \
          \         logger.info(f\"Wrote JSONL to {jsonl_path} via manual dump\")\n\
          \    except Exception as _e:\n        logger.warning(f\"Failed to export\
          \ JSONL dataset at {resolved_dataset_dir}: {_e}\")\n        # Leave jsonl_path\
          \ as default; downstream will fallback to directory if file not present\n\
          \n    # ------------------------------\n    # Training (placeholder for\
          \ TrainingHubTrainer)\n    # ------------------------------\n    checkpoints_dir\
          \ = os.path.join(pvc_path, \"checkpoints\")\n    os.makedirs(checkpoints_dir,\
          \ exist_ok=True)\n\n    # Wire in TrainingHubTrainer (modularized steps)\n\
          \    try:\n        from kubeflow.trainer import TrainerClient\n        from\
          \ kubeflow.trainer.rhai import TrainingHubAlgorithms, TrainingHubTrainer\n\
          \        from kubeflow_trainer_api import models as _th_models  # noqa:\
          \ F401\n        from kubeflow.common.types import KubernetesBackendConfig\n\
          \        from kubeflow.trainer.options.kubernetes import (\n           \
          \ PodTemplateOverrides,\n            PodTemplateOverride,\n            PodSpecOverride,\n\
          \            ContainerOverride,\n        )\n\n        if _api_client is\
          \ None:\n            raise RuntimeError(\"Kubernetes API client is not initialized\"\
          )\n\n        backend_cfg = KubernetesBackendConfig(client_configuration=_api_client.configuration)\n\
          \        client = TrainerClient(backend_cfg)\n\n        def _select_runtime(_client)\
          \ -> object:\n            \"\"\"Return the 'training-hub' runtime from Trainer\
          \ backend.\"\"\"\n            for rt in _client.list_runtimes():\n     \
          \           if getattr(rt, \"name\", \"\") == \"training-hub\":\n      \
          \              logger.info(f\"Found runtime: {rt}\")\n                 \
          \   return rt\n            raise RuntimeError(\"Training runtime 'training-hub'\
          \ not found\")\n\n        th_runtime = _select_runtime(client)\n\n     \
          \   # Build training parameters (aligned to OSFT/SFT)\n        parsed_target_patterns\
          \ = [p.strip() for p in training_target_patterns.split(\",\") if p.strip()]\
          \ if training_target_patterns else None\n        parsed_lr_sched_kwargs\
          \ = None\n        if training_lr_scheduler_kwargs:\n            try:\n \
          \               items = [s.strip() for s in training_lr_scheduler_kwargs.split(\"\
          ,\") if s.strip()]\n                kv: Dict[str, str] = {}\n          \
          \      for item in items:\n                    if \"=\" not in item:\n \
          \                       raise ValueError(\n                            f\"\
          Invalid scheduler kwargs segment '{item}'. Expected key=value.\"\n     \
          \                   )\n                    key, value = item.split(\"=\"\
          , 1)\n                    key = key.strip()\n                    value =\
          \ value.strip()\n                    if not key:\n                     \
          \   raise ValueError(\"Empty key in training_lr_scheduler_kwargs\")\n  \
          \                  kv[key] = value\n                parsed_lr_sched_kwargs\
          \ = kv\n            except Exception as e:\n                raise ValueError(f\"\
          Invalid training_lr_scheduler_kwargs format: {e}\")\n\n        def _build_params()\
          \ -> Dict[str, object]:\n            \"\"\"Build OSFT/SFT parameter set\
          \ for TrainingHub.\"\"\"\n            base = {\n                \"model_path\"\
          : training_base_model,\n                # Prefer JSONL export when available;\
          \ fallback to resolved directory\n                \"data_path\": jsonl_path\
          \ if os.path.exists(jsonl_path) else resolved_dataset_dir,\n           \
          \     \"effective_batch_size\": int(training_effective_batch_size if training_effective_batch_size\
          \ is not None else 128),\n                \"max_tokens_per_gpu\": int(training_max_tokens_per_gpu),\n\
          \                \"max_seq_len\": int(training_max_seq_len if training_max_seq_len\
          \ is not None else 8192),\n                \"learning_rate\": float(training_learning_rate\
          \ if training_learning_rate is not None else 5e-6),\n                \"\
          backend\": training_backend,\n                \"ckpt_output_dir\": checkpoints_dir,\n\
          \                \"data_output_dir\": training_data_output_dir or os.path.join(checkpoints_dir,\
          \ \"_internal_data_processing\"),\n                \"target_patterns\":\
          \ parsed_target_patterns or [],\n                \"seed\": int(training_seed)\
          \ if training_seed is not None else 42,\n                \"use_liger\":\
          \ bool(training_use_liger) if training_use_liger is not None else False,\n\
          \                \"use_processed_dataset\": bool(training_use_processed_dataset)\
          \ if training_use_processed_dataset is not None else False,\n          \
          \      \"unmask_messages\": bool(training_unmask_messages) if training_unmask_messages\
          \ is not None else False,\n                \"lr_scheduler\": training_lr_scheduler\
          \ or \"constant\",\n                \"warmup_steps\": int(training_lr_warmup_steps)\
          \ if training_lr_warmup_steps is not None else 0,\n                \"save_samples\"\
          : int(training_save_samples) if training_save_samples is not None else 0,\n\
          \                \"accelerate_full_state_at_epoch\": bool(training_accelerate_full_state_at_epoch)\
          \ if training_accelerate_full_state_at_epoch is not None else False,\n \
          \               \"lr_scheduler_kwargs\": parsed_lr_sched_kwargs or {},\n\
          \                \"checkpoint_at_epoch\": bool(training_checkpoint_at_epoch)\
          \ if training_checkpoint_at_epoch is not None else False,\n            \
          \    \"save_final_checkpoint\": bool(training_save_final_checkpoint) if\
          \ training_save_final_checkpoint is not None else False,\n             \
          \   \"num_epochs\": int(training_num_epochs) if training_num_epochs is not\
          \ None else 1,\n            }\n            if (training_algorithm or \"\"\
          ).strip().upper() == \"OSFT\":\n                base[\"unfreeze_rank_ratio\"\
          ] = float(training_unfreeze_rank_ratio)\n            return base\n\n   \
          \     params = _build_params()\n\n        # Algorithm selection: include\
          \ OSFT-only param when applicable\n        algo_value = TrainingHubAlgorithms.OSFT\
          \ if (training_algorithm or \"\").strip().upper() != \"SFT\" else TrainingHubAlgorithms.SFT\n\
          \        if algo_value == TrainingHubAlgorithms.OSFT:\n            params[\"\
          unfreeze_rank_ratio\"] = float(training_unfreeze_rank_ratio)\n\n       \
          \ # Build volumes and mounts (from passthrough only); do not inject env\
          \ via pod overrides\n        # Cluster policy forbids env in podTemplateOverrides;\
          \ use trainer.env for container env\n\n        volumes = []\n        volume_mounts\
          \ = []\n        if kubernetes_config and getattr(kubernetes_config, \"volumes\"\
          , None):\n            volumes.extend(kubernetes_config.volumes)\n      \
          \  if kubernetes_config and getattr(kubernetes_config, \"volume_mounts\"\
          , None):\n            volume_mounts.extend(kubernetes_config.volume_mounts)\n\
          \n        # Container resources are not overridden here; rely on runtime\
          \ defaults or future API support\n\n        # Parse metadata labels/annotations\
          \ for Pod template\n        tpl_labels = parse_kv_list(training_metadata_labels)\n\
          \        tpl_annotations = parse_kv_list(training_metadata_annotations)\n\
          \n        def _build_pod_spec_override() -> PodSpecOverride:\n         \
          \   \"\"\"Return PodSpecOverride with mounts, envs, resources, and scheduling\
          \ hints.\"\"\"\n            return PodSpecOverride(\n                volumes=volumes,\n\
          \                containers=[\n                    ContainerOverride(\n\
          \                        name=\"node\",\n                        volume_mounts=volume_mounts,\n\
          \                    )\n                ],\n                # node_selector=(kubernetes_config.node_selector\
          \ if kubernetes_config and getattr(kubernetes_config, \"node_selector\"\
          , None) else None),\n                # tolerations=(kubernetes_config.tolerations\
          \ if kubernetes_config and getattr(kubernetes_config, \"tolerations\", None)\
          \ else None),\n            )\n\n        job_name = client.train(\n     \
          \       trainer=TrainingHubTrainer(\n                algorithm=TrainingHubAlgorithms.OSFT\
          \ if (training_algorithm or \"\").strip().upper() != \"SFT\" else TrainingHubAlgorithms.SFT,\n\
          \                func_args=params,\n                packages_to_install=[],\n\
          \                # Pass environment variables via Trainer spec (allowed\
          \ by backend/webhook)\n                env=dict(merged_env),\n         \
          \   ),\n            options=[\n                PodTemplateOverrides(\n \
          \                   PodTemplateOverride(\n                        target_jobs=[\"\
          node\"],\n                        metadata={\"labels\": tpl_labels, \"annotations\"\
          : tpl_annotations} if (tpl_labels or tpl_annotations) else None,\n     \
          \                   spec=_build_pod_spec_override(),\n                 \
          \       # numProcsPerWorker=training_resource_num_procs_per_worker,\n  \
          \                      # numWorkers=training_resource_num_workers,\n   \
          \                 )\n                )\n            ],\n            runtime=th_runtime,\n\
          \        )\n        logger.info(f\"Submitted TrainingHub job: {job_name}\"\
          )\n        try:\n            # Wait for the job to start running, then wait\
          \ for completion or failure.\n            client.wait_for_job_status(name=job_name,\
          \ status={\"Running\"}, timeout=300)\n            client.wait_for_job_status(name=job_name,\
          \ status={\"Complete\", \"Failed\"}, timeout=1800)\n            job = client.get_job(name=job_name)\n\
          \            if getattr(job, \"status\", None) == \"Failed\":\n        \
          \        logger.error(\"Training job failed\")\n                raise RuntimeError(f\"\
          Training job failed with status: {job.status}\")\n            elif getattr(job,\
          \ \"status\", None) == \"Complete\":\n                logger.info(\"Training\
          \ job completed successfully\")\n            else:\n                logger.error(f\"\
          Unexpected training job status: {job.status}\")\n                raise RuntimeError(f\"\
          Training job ended with unexpected status: {job.status}\")\n        except\
          \ Exception as e:\n            logger.warning(f\"Training job monitoring\
          \ failed: {e}\")\n    except Exception as e:\n        logger.error(f\"TrainingHubTrainer\
          \ execution failed: {e}\")\n        raise\n\n    # ------------------------------\n\
          \    # Metrics (basic hyperparameters)\n    # ------------------------------\n\
          \    def _log_basic_metrics() -> None:\n        output_metrics.log_metric(\"\
          num_epochs\", float(params.get(\"num_epochs\") or 1))\n        output_metrics.log_metric(\"\
          effective_batch_size\", float(params.get(\"effective_batch_size\") or 128))\n\
          \        output_metrics.log_metric(\"learning_rate\", float(params.get(\"\
          learning_rate\") or 5e-6))\n        output_metrics.log_metric(\"max_seq_len\"\
          , float(params.get(\"max_seq_len\") or 8192))\n        output_metrics.log_metric(\"\
          max_tokens_per_gpu\", float(params.get(\"max_tokens_per_gpu\") or 0))\n\
          \        output_metrics.log_metric(\"unfreeze_rank_ratio\", float(params.get(\"\
          unfreeze_rank_ratio\") or 0))\n\n    _log_basic_metrics()\n\n    # ------------------------------\n\
          \    # Export most recent checkpoint as model artifact (artifact store)\
          \ and PVC\n    # ------------------------------\n    def _persist_and_annotate()\
          \ -> None:\n        \"\"\"Copy latest checkpoint to PVC and artifact store,\
          \ then annotate output metadata.\"\"\"\n        latest = find_most_recent_checkpoint(checkpoints_dir)\n\
          \        if not latest:\n            raise RuntimeError(f\"No checkpoints\
          \ found under {checkpoints_dir}\")\n        # PVC copy\n        pvc_dir\
          \ = os.path.join(pvc_path, \"final_model\")\n        try:\n            if\
          \ os.path.exists(pvc_dir):\n                shutil.rmtree(pvc_dir)\n   \
          \         shutil.copytree(latest, pvc_dir, dirs_exist_ok=True)\n       \
          \     logger.info(f\"Copied checkpoint to PVC dir: {pvc_dir}\")\n      \
          \  except Exception as _e:\n            logger.warning(f\"Failed to copy\
          \ model to PVC dir {pvc_dir}: {_e}\")\n        # Artifact copy\n       \
          \ output_model.name = f\"{training_base_model}-checkpoint\"\n        shutil.copytree(latest,\
          \ output_model.path, dirs_exist_ok=True)\n        logger.info(f\"Exported\
          \ checkpoint from {latest} to artifact path {output_model.path}\")\n   \
          \     # Metadata\n        try:\n            output_model.metadata[\"model_name\"\
          ] = training_base_model\n            output_model.metadata[\"artifact_path\"\
          ] = output_model.path\n            output_model.metadata[\"pvc_model_dir\"\
          ] = pvc_dir\n            logger.info(\"Annotated output_model metadata with\
          \ pvc/artifact locations\")\n        except Exception as _e:\n         \
          \   logger.warning(f\"Failed to set output_model metadata: {_e}\")\n\n \
          \   _persist_and_annotate()\n\n    return \"training completed\"\n\n"
        image: quay.io/opendatahub/odh-training-th03-cuda128-torch28-py312-rhel9@sha256:84d05c5ef9dd3c6ff8173c93dca7e2e6a1cab290f416fb2c469574f89b8e6438
pipelineInfo:
  description: 'Skeleton pipeline with 4 stages sharing a PVC: dataset download, training,
    lm-eval, model registry'
  name: dist-train
root:
  dag:
    tasks:
      dataset-download:
        cachingOptions: {}
        componentRef:
          name: comp-dataset-download
        inputs:
          parameters:
            pvc_mount_path:
              runtimeValue:
                constant: '{{$.workspace_path}}'
            shared_log_file:
              componentInputParameter: shared_log_file
        taskInfo:
          name: dataset-download
      eval-lm-eval:
        cachingOptions: {}
        componentRef:
          name: comp-eval-lm-eval
        dependentTasks:
        - train-model
        inputs:
          parameters:
            pvc_mount_path:
              runtimeValue:
                constant: '{{$.workspace_path}}'
            shared_log_file:
              componentInputParameter: shared_log_file
        taskInfo:
          name: eval-lm-eval
      model-registry:
        cachingOptions: {}
        componentRef:
          name: comp-model-registry
        dependentTasks:
        - eval-lm-eval
        inputs:
          parameters:
            pvc_mount_path:
              runtimeValue:
                constant: '{{$.workspace_path}}'
            shared_log_file:
              componentInputParameter: shared_log_file
        taskInfo:
          name: model-registry
      train-model:
        cachingOptions: {}
        componentRef:
          name: comp-train-model
        dependentTasks:
        - dataset-download
        inputs:
          parameters:
            pvc_path:
              runtimeValue:
                constant: '{{$.workspace_path}}'
            training_accelerate_full_state_at_epoch:
              componentInputParameter: training_accelerate_full_state_at_epoch
            training_algorithm:
              componentInputParameter: training_algorithm
            training_backend:
              componentInputParameter: training_backend
            training_base_model:
              componentInputParameter: training_base_model
            training_checkpoint_at_epoch:
              componentInputParameter: training_checkpoint_at_epoch
            training_effective_batch_size:
              componentInputParameter: training_effective_batch_size
            training_envs:
              componentInputParameter: training_envs
            training_learning_rate:
              componentInputParameter: training_learning_rate
            training_lr_scheduler:
              componentInputParameter: training_lr_scheduler
            training_lr_scheduler_kwargs:
              componentInputParameter: training_lr_scheduler_kwargs
            training_lr_warmup_steps:
              componentInputParameter: training_lr_warmup_steps
            training_max_seq_len:
              componentInputParameter: training_max_seq_len
            training_max_tokens_per_gpu:
              componentInputParameter: training_max_tokens_per_gpu
            training_metadata_annotations:
              componentInputParameter: training_metadata_annotations
            training_metadata_labels:
              componentInputParameter: training_metadata_labels
            training_num_epochs:
              componentInputParameter: training_num_epochs
            training_resource_cpu_per_worker:
              componentInputParameter: training_resource_cpu_per_worker
            training_resource_gpu_per_worker:
              componentInputParameter: training_resource_gpu_per_worker
            training_resource_memory_per_worker:
              componentInputParameter: training_resource_memory_per_worker
            training_resource_num_procs_per_worker:
              componentInputParameter: training_resource_num_procs_per_worker
            training_resource_num_workers:
              componentInputParameter: training_resource_num_workers
            training_save_final_checkpoint:
              componentInputParameter: training_save_final_checkpoint
            training_save_samples:
              componentInputParameter: training_save_samples
            training_seed:
              componentInputParameter: training_seed
            training_target_patterns:
              componentInputParameter: training_target_patterns
            training_unfreeze_rank_ratio:
              componentInputParameter: training_unfreeze_rank_ratio
            training_unmask_messages:
              componentInputParameter: training_unmask_messages
            training_use_liger:
              componentInputParameter: training_use_liger
            training_use_processed_dataset:
              componentInputParameter: training_use_processed_dataset
        taskInfo:
          name: train-model
  inputDefinitions:
    parameters:
      shared_log_file:
        defaultValue: pipeline_log.txt
        description: Name of the shared log file for tracking completion.
        isOptional: true
        parameterType: STRING
      training_accelerate_full_state_at_epoch:
        isOptional: true
        parameterType: BOOLEAN
      training_algorithm:
        defaultValue: OSFT
        isOptional: true
        parameterType: STRING
      training_backend:
        defaultValue: mini-trainer
        isOptional: true
        parameterType: STRING
      training_base_model:
        defaultValue: Qwen/Qwen2.5-1.5B-Instruct
        isOptional: true
        parameterType: STRING
      training_checkpoint_at_epoch:
        defaultValue: false
        isOptional: true
        parameterType: BOOLEAN
      training_effective_batch_size:
        defaultValue: 128.0
        isOptional: true
        parameterType: NUMBER_INTEGER
      training_envs:
        defaultValue: ''
        isOptional: true
        parameterType: STRING
      training_learning_rate:
        defaultValue: 5.0e-06
        isOptional: true
        parameterType: NUMBER_DOUBLE
      training_lr_scheduler:
        defaultValue: cosine
        isOptional: true
        parameterType: STRING
      training_lr_scheduler_kwargs:
        defaultValue: ''
        isOptional: true
        parameterType: STRING
      training_lr_warmup_steps:
        defaultValue: 0.0
        isOptional: true
        parameterType: NUMBER_INTEGER
      training_max_seq_len:
        defaultValue: 8192.0
        isOptional: true
        parameterType: NUMBER_INTEGER
      training_max_tokens_per_gpu:
        defaultValue: 64000.0
        isOptional: true
        parameterType: NUMBER_INTEGER
      training_metadata_annotations:
        defaultValue: ''
        isOptional: true
        parameterType: STRING
      training_metadata_labels:
        defaultValue: ''
        isOptional: true
        parameterType: STRING
      training_num_epochs:
        defaultValue: 1.0
        isOptional: true
        parameterType: NUMBER_INTEGER
      training_resource_cpu_per_worker:
        defaultValue: '8'
        isOptional: true
        parameterType: STRING
      training_resource_gpu_per_worker:
        defaultValue: 1.0
        isOptional: true
        parameterType: NUMBER_INTEGER
      training_resource_memory_per_worker:
        defaultValue: 32Gi
        isOptional: true
        parameterType: STRING
      training_resource_num_procs_per_worker:
        defaultValue: 1.0
        isOptional: true
        parameterType: NUMBER_INTEGER
      training_resource_num_workers:
        defaultValue: 1.0
        isOptional: true
        parameterType: NUMBER_INTEGER
      training_resources_num_nodes:
        defaultValue: 2.0
        isOptional: true
        parameterType: NUMBER_INTEGER
      training_save_final_checkpoint:
        defaultValue: true
        isOptional: true
        parameterType: BOOLEAN
      training_save_samples:
        isOptional: true
        parameterType: NUMBER_INTEGER
      training_seed:
        defaultValue: 42.0
        isOptional: true
        parameterType: NUMBER_INTEGER
      training_target_patterns:
        defaultValue: ''
        isOptional: true
        parameterType: STRING
      training_unfreeze_rank_ratio:
        defaultValue: 0.25
        isOptional: true
        parameterType: NUMBER_DOUBLE
      training_unmask_messages:
        isOptional: true
        parameterType: BOOLEAN
      training_use_liger:
        defaultValue: true
        isOptional: true
        parameterType: BOOLEAN
      training_use_processed_dataset:
        isOptional: true
        parameterType: BOOLEAN
schemaVersion: 2.1.0
sdkVersion: kfp-2.15.2
---
platforms:
  kubernetes:
    deploymentSpec:
      executors:
        exec-dataset-download:
          imagePullPolicy: IfNotPresent
        exec-eval-lm-eval:
          imagePullPolicy: IfNotPresent
        exec-model-registry:
          imagePullPolicy: IfNotPresent
        exec-train-model:
          imagePullPolicy: IfNotPresent
          secretAsEnv:
          - keyToEnv:
            - envVar: KUBERNETES_SERVER_URL
              secretKey: server_url
            - envVar: KUBERNETES_AUTH_TOKEN
              secretKey: auth_token
            optional: false
            secretName: kubernetes-credentials
            secretNameParameter:
              runtimeValue:
                constant: kubernetes-credentials
    pipelineConfig:
      workspace:
        kubernetes:
          pvcSpecPatch:
            accessModes:
            - ReadWriteMany
            storageClassName: nfs-csi
        size: 10Gi
