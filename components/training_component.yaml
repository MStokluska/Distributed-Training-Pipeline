# PIPELINE DEFINITION
# Name: train-model
# Description: Perform model training (inline) using PVC workspace and TrainingHub runtime.
# Inputs:
#    dataset: system.Dataset
#    kubernetes_config: TaskConfig
#    pvc_path: str
#    training_accelerate_full_state_at_epoch: bool
#    training_algorithm: str [Default: 'OSFT']
#    training_backend: str [Default: 'mini-trainer']
#    training_base_model: str [Default: 'Qwen/Qwen2.5-1.5B-Instruct']
#    training_checkpoint_at_epoch: bool
#    training_data_output_dir: str
#    training_effective_batch_size: int [Default: 128.0]
#    training_envs: str [Default: '']
#    training_hf_token: str [Default: '']
#    training_learning_rate: float
#    training_lr_scheduler: str
#    training_lr_scheduler_kwargs: str [Default: '']
#    training_lr_warmup_steps: int
#    training_max_seq_len: int [Default: 8192.0]
#    training_max_tokens_per_gpu: int [Default: 64000.0]
#    training_metadata_annotations: str [Default: '']
#    training_metadata_labels: str [Default: '']
#    training_num_epochs: int
#    training_resource_cpu_per_worker: str [Default: '8']
#    training_resource_gpu_per_worker: int [Default: 1.0]
#    training_resource_memory_per_worker: str [Default: '32Gi']
#    training_resource_num_procs_per_worker: int [Default: 1.0]
#    training_resource_num_workers: int [Default: 1.0]
#    training_save_final_checkpoint: bool
#    training_save_samples: int
#    training_seed: int
#    training_target_patterns: str [Default: '']
#    training_unfreeze_rank_ratio: float [Default: 0.25]
#    training_unmask_messages: bool
#    training_use_liger: bool
#    training_use_processed_dataset: bool
# Outputs:
#    Output: str
#    output_metrics: system.Metrics
#    output_model: system.Model
components:
  comp-train-model:
    executorLabel: exec-train-model
    inputDefinitions:
      artifacts:
        dataset:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
          description: 'Input dataset artifact (preferred). If not present, this component

            will attempt to load from a remote path specified in dataset.metadata.

            - metadata["artifact_path"]: remote dataset path (e.g., s3://..., https://...,
            or HF repo id)

            - metadata["pvc_dir"]: pre-staged PVC directory to use if present'
          isOptional: true
      parameters:
        kubernetes_config:
          description: TaskConfig passthrough (volumes, mounts, env, resources, tolerations,
            etc.).
          isOptional: true
          parameterType: TASK_CONFIG
        pvc_path:
          description: Root of the workspace PVC for this run.
          parameterType: STRING
        training_accelerate_full_state_at_epoch:
          description: Whether to save full Accelerate state at each epoch (optional).
          isOptional: true
          parameterType: BOOLEAN
        training_algorithm:
          defaultValue: OSFT
          description: Training algorithm ("OSFT" | "SFT"). OSFT adds continual learning
            support.
          isOptional: true
          parameterType: STRING
        training_backend:
          defaultValue: mini-trainer
          description: Trainer backend variant (e.g., "mini-trainer").
          isOptional: true
          parameterType: STRING
        training_base_model:
          defaultValue: Qwen/Qwen2.5-1.5B-Instruct
          description: HuggingFace model ID to fine-tune (e.g., "Qwen/Qwen2.5-1.5B-Instruct").
          isOptional: true
          parameterType: STRING
        training_checkpoint_at_epoch:
          description: Save a checkpoint at each epoch boundary.
          isOptional: true
          parameterType: BOOLEAN
        training_data_output_dir:
          description: Optional secondary output directory on PVC.
          isOptional: true
          parameterType: STRING
        training_effective_batch_size:
          defaultValue: 128.0
          description: "Per-step batch size. Guidance:\n- 1 GPU: 16\u201332\n- 2 GPUs:\
            \ 32\u201364\n- 4 GPUs: 64\u2013128"
          isOptional: true
          parameterType: NUMBER_INTEGER
        training_envs:
          defaultValue: ''
          description: Comma-separated env overrides ("KEY=VAL,KEY=VAL").
          isOptional: true
          parameterType: STRING
        training_hf_token:
          defaultValue: ''
          isOptional: true
          parameterType: STRING
        training_learning_rate:
          description: Learning rate (typ. 1e-6 to 1e-4; 5e-6 is a good OSFT default).
          isOptional: true
          parameterType: NUMBER_DOUBLE
        training_lr_scheduler:
          description: LR scheduler ("cosine" | "linear" | "constant").
          isOptional: true
          parameterType: STRING
        training_lr_scheduler_kwargs:
          defaultValue: ''
          description: 'Comma-delimited key=value string for scheduler kwargs

            (e.g., "num_cycles=1,num_warmup_steps=100").'
          isOptional: true
          parameterType: STRING
        training_lr_warmup_steps:
          description: LR warmup steps (0 for none).
          isOptional: true
          parameterType: NUMBER_INTEGER
        training_max_seq_len:
          defaultValue: 8192.0
          description: "Max sequence length (typical 2048\u20138192)."
          isOptional: true
          parameterType: NUMBER_INTEGER
        training_max_tokens_per_gpu:
          defaultValue: 64000.0
          description: Token budget per GPU for memory mgmt.
          isOptional: true
          parameterType: NUMBER_INTEGER
        training_metadata_annotations:
          defaultValue: ''
          description: Comma-separated annotations ("k=v,k=v") for pod template.
          isOptional: true
          parameterType: STRING
        training_metadata_labels:
          defaultValue: ''
          description: Comma-separated labels ("k=v,k=v") for pod template.
          isOptional: true
          parameterType: STRING
        training_num_epochs:
          description: "Number of epochs (1 = quick test; 3\u20135 = better convergence)."
          isOptional: true
          parameterType: NUMBER_INTEGER
        training_resource_cpu_per_worker:
          defaultValue: '8'
          description: CPU limit/request per worker (e.g., "8").
          isOptional: true
          parameterType: STRING
        training_resource_gpu_per_worker:
          defaultValue: 1.0
          description: GPUs per worker (e.g., 1). Typically equals num procs.
          isOptional: true
          parameterType: NUMBER_INTEGER
        training_resource_memory_per_worker:
          defaultValue: 32Gi
          description: Memory per worker (e.g., "32Gi").
          isOptional: true
          parameterType: STRING
        training_resource_num_procs_per_worker:
          defaultValue: 1.0
          description: Processes (ranks) per worker (usually equals GPUs/worker).
          isOptional: true
          parameterType: NUMBER_INTEGER
        training_resource_num_workers:
          defaultValue: 1.0
          description: Total worker pods (1 = single-node; 2+ = multi-node).
          isOptional: true
          parameterType: NUMBER_INTEGER
        training_save_final_checkpoint:
          description: Save the final model checkpoint.
          isOptional: true
          parameterType: BOOLEAN
        training_save_samples:
          description: Number of samples to save during SFT (optional).
          isOptional: true
          parameterType: NUMBER_INTEGER
        training_seed:
          description: Random seed for reproducibility.
          isOptional: true
          parameterType: NUMBER_INTEGER
        training_target_patterns:
          defaultValue: ''
          description: Comma-separated target modules/patterns (algorithm-specific).
          isOptional: true
          parameterType: STRING
        training_unfreeze_rank_ratio:
          defaultValue: 0.25
          isOptional: true
          parameterType: NUMBER_DOUBLE
        training_unmask_messages:
          description: Whether to unmask chat messages if applicable.
          isOptional: true
          parameterType: BOOLEAN
        training_use_liger:
          description: Enable Liger kernel optimizations (image must include kernels).
          isOptional: true
          parameterType: BOOLEAN
        training_use_processed_dataset:
          description: Whether dataset is already processed.
          isOptional: true
          parameterType: BOOLEAN
    outputDefinitions:
      artifacts:
        output_metrics:
          artifactType:
            schemaTitle: system.Metrics
            schemaVersion: 0.0.1
        output_model:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
      parameters:
        Output:
          parameterType: STRING
    taskConfigPassthroughs:
    - field: RESOURCES
    - field: KUBERNETES_TOLERATIONS
    - field: KUBERNETES_NODE_SELECTOR
    - field: KUBERNETES_AFFINITY
    - applyToTask: true
      field: ENV
    - applyToTask: true
      field: KUBERNETES_VOLUMES
deploymentSpec:
  executors:
    exec-train-model:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - train_model
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kubernetes'\
          \ 'olot'  &&  python3 -m pip install --quiet --no-warn-script-location 'kfp==2.15.2'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"' && \"\
          $0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef train_model(\n    # Workspace/PVC root (pass dsl.WORKSPACE_PATH_PLACEHOLDER\
          \ at call site)\n    pvc_path: str,\n    # Outputs (no defaults)\n    output_model:\
          \ dsl.Output[dsl.Model],\n    output_metrics: dsl.Output[dsl.Metrics],\n\
          \    # Dataset input and optional remote artifact path via metadata (e.g.,\
          \ s3://...)\n    dataset: dsl.Input[dsl.Dataset] = None,\n    # Base model\
          \ (HF ID or local path)\n    training_base_model: str = \"Qwen/Qwen2.5-1.5B-Instruct\"\
          ,\n    # Training algorithm selector\n    training_algorithm: str = \"OSFT\"\
          ,\n    # OSFT parameters (prefixed with training_)\n    training_unfreeze_rank_ratio:\
          \ float = 0.25,\n    training_effective_batch_size: int = 128,\n    training_max_tokens_per_gpu:\
          \ int = 64000,\n    training_max_seq_len: int = 8192,\n    training_learning_rate:\
          \ Optional[float] = None,\n    training_backend: str = \"mini-trainer\"\
          ,\n    training_target_patterns: str = \"\",\n    training_seed: Optional[int]\
          \ = None,\n    training_use_liger: Optional[bool] = None,\n    training_use_processed_dataset:\
          \ Optional[bool] = None,\n    training_unmask_messages: Optional[bool] =\
          \ None,\n    training_lr_scheduler: Optional[str] = None,\n    training_lr_warmup_steps:\
          \ Optional[int] = None,\n    training_save_samples: Optional[int] = None,\n\
          \    training_accelerate_full_state_at_epoch: Optional[bool] = None,\n \
          \   training_lr_scheduler_kwargs: str = \"\",\n    training_checkpoint_at_epoch:\
          \ Optional[bool] = None,\n    training_save_final_checkpoint: Optional[bool]\
          \ = None,\n    training_num_epochs: Optional[int] = None,\n    training_data_output_dir:\
          \ Optional[str] = None,\n    # HuggingFace token for gated models (optional\
          \ - leave empty if not needed)\n    training_hf_token: str = \"\",\n   \
          \ # Env overrides: \"KEY=VAL,KEY=VAL\"\n    training_envs: str = \"\",\n\
          \    # Resource and runtime parameters (per worker/pod)\n    training_resource_cpu_per_worker:\
          \ str = \"8\",\n    training_resource_gpu_per_worker: int = 1,\n    training_resource_memory_per_worker:\
          \ str = \"32Gi\",\n    training_resource_num_procs_per_worker: int = 1,\n\
          \    training_resource_num_workers: int = 1,\n    training_metadata_labels:\
          \ str = \"\",\n    training_metadata_annotations: str = \"\",\n    # KFP\
          \ TaskConfig passthrough for volumes/env/resources, etc.\n    kubernetes_config:\
          \ dsl.TaskConfig = None,\n) -> str:\n    \"\"\"Perform model training (inline)\
          \ using PVC workspace and TrainingHub runtime.\n\n    Args:\n        pvc_path:\
          \ Root of the workspace PVC for this run.\n        dataset: Input dataset\
          \ artifact (preferred). If not present, this component\n            will\
          \ attempt to load from a remote path specified in dataset.metadata.\n  \
          \          - metadata[\"artifact_path\"]: remote dataset path (e.g., s3://...,\
          \ https://..., or HF repo id)\n            - metadata[\"pvc_dir\"]: pre-staged\
          \ PVC directory to use if present\n        training_base_model: HuggingFace\
          \ model ID to fine-tune (e.g., \"Qwen/Qwen2.5-1.5B-Instruct\").\n\n    \
          \    training_algorithm: Training algorithm (\"OSFT\" | \"SFT\"). OSFT adds\
          \ continual learning support.\n        training_effective_batch_size: Per-step\
          \ batch size. Guidance:\n            - 1 GPU: 16\u201332\n            -\
          \ 2 GPUs: 32\u201364\n            - 4 GPUs: 64\u2013128\n        training_max_tokens_per_gpu:\
          \ Token budget per GPU for memory mgmt.\n        training_max_seq_len: Max\
          \ sequence length (typical 2048\u20138192).\n        training_learning_rate:\
          \ Learning rate (typ. 1e-6 to 1e-4; 5e-6 is a good OSFT default).\n    \
          \    training_backend: Trainer backend variant (e.g., \"mini-trainer\").\n\
          \        training_target_patterns: Comma-separated target modules/patterns\
          \ (algorithm-specific).\n        training_seed: Random seed for reproducibility.\n\
          \        training_use_liger: Enable Liger kernel optimizations (image must\
          \ include kernels).\n        training_use_processed_dataset: Whether dataset\
          \ is already processed.\n        training_unmask_messages: Whether to unmask\
          \ chat messages if applicable.\n        training_lr_scheduler: LR scheduler\
          \ (\"cosine\" | \"linear\" | \"constant\").\n        training_lr_warmup_steps:\
          \ LR warmup steps (0 for none).\n        training_save_samples: Number of\
          \ samples to save during SFT (optional).\n        training_accelerate_full_state_at_epoch:\
          \ Whether to save full Accelerate state at each epoch (optional).\n    \
          \    training_lr_scheduler_kwargs: Comma-delimited key=value string for\
          \ scheduler kwargs\n            (e.g., \"num_cycles=1,num_warmup_steps=100\"\
          ).\n        training_checkpoint_at_epoch: Save a checkpoint at each epoch\
          \ boundary.\n        training_save_final_checkpoint: Save the final model\
          \ checkpoint.\n        training_num_epochs: Number of epochs (1 = quick\
          \ test; 3\u20135 = better convergence).\n        training_data_output_dir:\
          \ Optional secondary output directory on PVC.\n\n        training_envs:\
          \ Comma-separated env overrides (\"KEY=VAL,KEY=VAL\").\n        training_resource_cpu_per_worker:\
          \ CPU limit/request per worker (e.g., \"8\").\n        training_resource_gpu_per_worker:\
          \ GPUs per worker (e.g., 1). Typically equals num procs.\n        training_resource_memory_per_worker:\
          \ Memory per worker (e.g., \"32Gi\").\n        training_resource_num_procs_per_worker:\
          \ Processes (ranks) per worker (usually equals GPUs/worker).\n        training_resource_num_workers:\
          \ Total worker pods (1 = single-node; 2+ = multi-node).\n        training_metadata_labels:\
          \ Comma-separated labels (\"k=v,k=v\") for pod template.\n        training_metadata_annotations:\
          \ Comma-separated annotations (\"k=v,k=v\") for pod template.\n        kubernetes_config:\
          \ TaskConfig passthrough (volumes, mounts, env, resources, tolerations,\
          \ etc.).\n\n        output_model: Final model artifact copied to artifact\
          \ store and PVC,\n            with metadata set for downstream consumers:\n\
          \            - model_name: the fine-tuned base model id\n            - artifact_path:\
          \ output_model.path (artifact store path)\n            - pvc_model_dir:\
          \ \"<pvc_path>/final_model\" (PVC directory path)\n        output_metrics:\
          \ Logged numeric metrics (floats), e.g.:\n            - num_epochs, effective_batch_size,\
          \ learning_rate, max_seq_len\n            - max_tokens_per_gpu, unfreeze_rank_ratio\
          \ (0 for SFT)\n\n    OSFT func_args schema (passed to the trainer):\n\n\
          \        model_path: Path to the model to fine-tune\n\n        data_path:\
          \ Path to the training data\n\n        ckpt_output_dir: Directory to save\
          \ checkpoints\n\n        backend: Backend implementation to use (default:\
          \ \"instructlab-training\")\n\n        num_epochs: Number of training epochs\n\
          \n        effective_batch_size: Effective batch size for training\n\n  \
          \      learning_rate: Learning rate for training\n\n        max_seq_len:\
          \ Maximum sequence length\n\n        max_tokens_per_gpu: Maximum tokens\
          \ per GPU in a mini-batch (hard-cap for memory to avoid OOMs). Used to automatically\
          \ calculate mini-batch size and gradient accumulation to maintain the desired\
          \ effective_batch_size while staying within memory limits.\n\n        data_output_dir:\
          \ Directory to save processed data\n\n        save_samples: Number of samples\
          \ to save after training (0 disables saving based on sample count)\n\n \
          \       warmup_steps: Number of warmup steps\n\n        accelerate_full_state_at_epoch:\
          \ Whether to save full state at epoch for automatic checkpoint resumption\n\
          \n        checkpoint_at_epoch: Whether to checkpoint at each epoch\n\n \
          \   Returns:\n        Status message string.\n    \"\"\"\n    import os,\
          \ sys, json, time, logging, re, subprocess, shutil\n    from typing import\
          \ Dict, List, Tuple, Optional as _Optional\n\n    # ------------------------------\n\
          \    # Logging configuration\n    # ------------------------------\n   \
          \ def _setup_logger() -> logging.Logger:\n        \"\"\"Configure and return\
          \ a logger for this component.\"\"\"\n        _logger = logging.getLogger(\"\
          train_model\")\n        _logger.setLevel(logging.INFO)\n        if not _logger.handlers:\n\
          \            _ch = logging.StreamHandler(sys.stdout)\n            _ch.setLevel(logging.INFO)\n\
          \            _ch.setFormatter(logging.Formatter(\"%(asctime)s - %(levelname)s\
          \ - %(message)s\"))\n            _logger.addHandler(_ch)\n        return\
          \ _logger\n\n    logger = _setup_logger()\n    logger.info(\"Initializing\
          \ training component\")\n    logger.info(f\"pvc_path={pvc_path}, model_name={training_base_model}\"\
          )\n\n    # ------------------------------\n    # Utility: find model directory\
          \ (with config.json)\n    # ------------------------------\n    def find_model_directory(checkpoints_root:\
          \ str) -> _Optional[str]:\n        \"\"\"Find the actual model directory\
          \ containing config.json.\n\n        Searches recursively for a directory\
          \ with config.json, prioritizing\n        the most recently modified one.\
          \ Handles nested checkpoint structures\n        like: checkpoints/epoch-1/samples_90.0/config.json\n\
          \        \"\"\"\n        if not os.path.isdir(checkpoints_root):\n     \
          \       return None\n\n        candidates: list = []\n        for root,\
          \ dirs, files in os.walk(checkpoints_root):\n            if \"config.json\"\
          \ in files:\n                try:\n                    mtime = os.path.getmtime(os.path.join(root,\
          \ \"config.json\"))\n                    candidates.append((mtime, root))\n\
          \                except OSError:\n                    continue\n\n     \
          \   if not candidates:\n            # Fallback: return most recent top-level\
          \ directory\n            latest: _Optional[Tuple[float, str]] = None\n \
          \           for entry in os.listdir(checkpoints_root):\n               \
          \ full = os.path.join(checkpoints_root, entry)\n                if os.path.isdir(full):\n\
          \                    try:\n                        mtime = os.path.getmtime(full)\n\
          \                    except OSError:\n                        continue\n\
          \                    if latest is None or mtime > latest[0]:\n         \
          \               latest = (mtime, full)\n            return latest[1] if\
          \ latest else None\n\n        # Return the most recently modified model\
          \ directory\n        candidates.sort(reverse=True)\n        return candidates[0][1]\n\
          \n    # ------------------------------\n    # Kubernetes connection\n  \
          \  # ------------------------------\n    def _init_k8s_client() -> _Optional[\"\
          k8s_client.ApiClient\"]:\n        \"\"\"Initialize and return a Kubernetes\
          \ client from env (server/token) or in-cluster/kubeconfig.\"\"\"\n     \
          \   try:\n            from kubernetes import client as k8s_client, config\
          \ as k8s_config\n            env_server = os.environ.get(\"KUBERNETES_SERVER_URL\"\
          , \"\").strip()\n            env_token = os.environ.get(\"KUBERNETES_AUTH_TOKEN\"\
          , \"\").strip()\n            if env_server and env_token:\n            \
          \    logger.info(\"Configuring Kubernetes client from env (KUBERNETES_SERVER_URL/_AUTH_TOKEN)\"\
          )\n                cfg = k8s_client.Configuration()\n                cfg.host\
          \ = env_server\n                cfg.verify_ssl = False\n               \
          \ cfg.api_key = {\"authorization\": f\"Bearer {env_token}\"}\n         \
          \       k8s_client.Configuration.set_default(cfg)\n                return\
          \ k8s_client.ApiClient(cfg)\n            logger.info(\"Configuring Kubernetes\
          \ client in-cluster (or local kubeconfig)\")\n            try:\n       \
          \         k8s_config.load_incluster_config()\n            except Exception:\n\
          \                k8s_config.load_kube_config()\n            return k8s_client.ApiClient()\n\
          \        except Exception as _exc:\n            logger.warning(f\"Kubernetes\
          \ client not initialized: {_exc}\")\n            return None\n\n    _api_client\
          \ = _init_k8s_client()\n\n    # ------------------------------\n    # Environment\
          \ variables (defaults + overrides)\n    # ------------------------------\n\
          \    cache_root = os.path.join(pvc_path, \".cache\", \"huggingface\")\n\
          \    default_env: Dict[str, str] = {\n        \"XDG_CACHE_HOME\": \"/tmp\"\
          ,\n        \"TRITON_CACHE_DIR\": \"/tmp/.triton\",\n        \"HF_HOME\"\
          : \"/tmp/.cache/huggingface\",\n        \"HF_DATASETS_CACHE\": os.path.join(cache_root,\
          \ \"datasets\"),\n        \"TRANSFORMERS_CACHE\": os.path.join(cache_root,\
          \ \"transformers\"),\n        \"NCCL_DEBUG\": \"INFO\",\n    }\n\n    def\
          \ parse_kv_list(kv_str: str) -> Dict[str, str]:\n        out: Dict[str,\
          \ str] = {}\n        if not kv_str:\n            return out\n        for\
          \ item in kv_str.split(\",\"):\n            item = item.strip()\n      \
          \      if not item:\n                continue\n            if \"=\" not\
          \ in item:\n                raise ValueError(f\"Invalid key=value item (expected\
          \ key=value): {item}\")\n            k, v = item.split(\"=\", 1)\n     \
          \       k = k.strip()\n            v = v.strip()\n            if not k:\n\
          \                raise ValueError(f\"Invalid key in key=value pair: {item}\"\
          )\n            out[k] = v\n        return out\n\n    def _configure_env(env_csv:\
          \ str, base_env: Dict[str, str]) -> Dict[str, str]:\n        \"\"\"Merge\
          \ base env with CSV overrides and export them to process env; return merged\
          \ map.\"\"\"\n        overrides = parse_kv_list(env_csv)\n        merged\
          \ = {**base_env, **overrides}\n        for ek, ev in merged.items():\n \
          \           os.environ[ek] = ev\n        logger.info(f\"Env configured (keys):\
          \ {sorted(list(merged.keys()))}\")\n        return merged\n\n    merged_env\
          \ = _configure_env(training_envs, default_env)\n\n    # Add HuggingFace\
          \ token to environment if provided\n    if training_hf_token and training_hf_token.strip():\n\
          \        merged_env[\"HF_TOKEN\"] = training_hf_token.strip()\n        os.environ[\"\
          HF_TOKEN\"] = training_hf_token.strip()\n        logger.info(\"HF_TOKEN\
          \ added to environment (for gated model access)\")\n\n    # ------------------------------\n\
          \    # Dataset resolution\n    # ------------------------------\n    from\
          \ datasets import load_dataset, load_from_disk, Dataset\n\n    resolved_dataset_dir\
          \ = os.path.join(pvc_path, \"dataset\", \"train\")\n    os.makedirs(resolved_dataset_dir,\
          \ exist_ok=True)\n\n    def is_local_path(p: str) -> bool:\n        return\
          \ bool(p) and os.path.exists(p)\n\n    def looks_like_url(p: str) -> bool:\n\
          \        return p.startswith(\"s3://\") or p.startswith(\"http://\") or\
          \ p.startswith(\"https://\")\n\n    def _resolve_dataset(input_dataset:\
          \ _Optional[dsl.Input[dsl.Dataset]], out_dir: str) -> None:\n        \"\"\
          \"Resolve dataset with preference: existing PVC dir > input artifact > remote\
          \ artifact/HF > default.\n        Remote path is read from input_dataset.metadata['artifact_path']\
          \ if present. If metadata['pvc_dir'] exists, prefer it.\n        \"\"\"\n\
          \        # 0) If already present (e.g., staged by prior step), keep it\n\
          \        if os.path.isdir(out_dir) and any(os.scandir(out_dir)):\n     \
          \       logger.info(f\"Using existing dataset at {out_dir}\")\n        \
          \    return\n        # 1) Input artifact (can be a file or directory)\n\
          \        if input_dataset and getattr(input_dataset, \"path\", None) and\
          \ os.path.exists(input_dataset.path):\n            src_path = input_dataset.path\n\
          \            if os.path.isdir(src_path):\n                logger.info(f\"\
          Copying input dataset directory from {src_path} to {out_dir}\")\n      \
          \          shutil.copytree(src_path, out_dir, dirs_exist_ok=True)\n    \
          \        else:\n                # It's a file (e.g., JSONL) - copy to out_dir\
          \ with appropriate name\n                logger.info(f\"Copying input dataset\
          \ file from {src_path} to {out_dir}\")\n                dst_file = os.path.join(out_dir,\
          \ os.path.basename(src_path))\n                # If basename doesn't have\
          \ extension, assume it's a jsonl file\n                if not os.path.splitext(dst_file)[1]:\n\
          \                    dst_file = os.path.join(out_dir, \"train.jsonl\")\n\
          \                shutil.copy2(src_path, dst_file)\n                logger.info(f\"\
          Dataset file copied to {dst_file}\")\n            return\n        # 2) Remote\
          \ artifact (S3/HTTP) or HF repo id\n        rp = \"\"\n        try:\n  \
          \          if input_dataset and hasattr(input_dataset, \"metadata\") and\
          \ isinstance(input_dataset.metadata, dict):\n                pvc_path_meta\
          \ = (input_dataset.metadata.get(\"pvc_path\") or input_dataset.metadata.get(\"\
          pvc_dir\") or \"\").strip()\n                if pvc_path_meta and os.path.exists(pvc_path_meta):\n\
          \                    if os.path.isdir(pvc_path_meta) and any(os.scandir(pvc_path_meta)):\n\
          \                        logger.info(f\"Using pre-staged PVC dataset directory\
          \ at {pvc_path_meta}\")\n                        shutil.copytree(pvc_path_meta,\
          \ out_dir, dirs_exist_ok=True)\n                        return\n       \
          \             elif os.path.isfile(pvc_path_meta):\n                    \
          \    logger.info(f\"Using pre-staged PVC dataset file at {pvc_path_meta}\"\
          )\n                        dst_file = os.path.join(out_dir, os.path.basename(pvc_path_meta))\n\
          \                        if not os.path.splitext(dst_file)[1]:\n       \
          \                     dst_file = os.path.join(out_dir, \"train.jsonl\")\n\
          \                        shutil.copy2(pvc_path_meta, dst_file)\n       \
          \                 return\n                rp = (input_dataset.metadata.get(\"\
          artifact_path\") or \"\").strip()\n        except Exception:\n         \
          \   rp = \"\"\n        if rp:\n            if looks_like_url(rp):\n    \
          \            logger.info(f\"Attempting to load remote dataset from {rp}\"\
          )\n                # Try a few common formats via datasets library\n   \
          \             ext = rp.lower()\n                try:\n                 \
          \   if ext.endswith(\".json\") or ext.endswith(\".jsonl\"):\n          \
          \              ds: Dataset = load_dataset(\"json\", data_files=rp, split=\"\
          train\")\n                    elif ext.endswith(\".parquet\"):\n       \
          \                 ds: Dataset = load_dataset(\"parquet\", data_files=rp,\
          \ split=\"train\")\n                    else:\n                        raise\
          \ ValueError(\n                            \"Unsupported remote dataset\
          \ format. Provide a JSON/JSONL/PARQUET file or a HF dataset repo id.\"\n\
          \                        )\n                    ds.save_to_disk(out_dir)\n\
          \                    return\n                except Exception as e:\n  \
          \                  raise ValueError(f\"Failed to load remote dataset from\
          \ {rp}: {e}\")\n            else:\n                # Treat as HF dataset\
          \ repo id\n                logger.info(f\"Assuming HF dataset repo id: {rp}\"\
          )\n                ds: Dataset = load_dataset(rp, split=\"train\")\n   \
          \             ds.save_to_disk(out_dir)\n                return\n       \
          \ # 3) Default fallback (Table-GPT)\n        logger.info(\"No dataset provided.\
          \ Falling back to 'LipengCS/Table-GPT'\")\n        ds: Dataset = load_dataset(\"\
          LipengCS/Table-GPT\", \"All\", split=\"train\")\n        ds.save_to_disk(out_dir)\n\
          \n    _resolve_dataset(dataset, resolved_dataset_dir)\n\n    # Export dataset\
          \ to JSONL so downstream trainer reads a plain JSONL file\n    jsonl_path\
          \ = os.path.join(resolved_dataset_dir, \"train.jsonl\")\n    try:\n    \
          \    # Try loading from the saved HF dataset on disk and export to JSONL\n\
          \        ds_on_disk = load_from_disk(resolved_dataset_dir)\n        # Handle\
          \ DatasetDict vs Dataset\n        train_split = ds_on_disk[\"train\"] if\
          \ isinstance(ds_on_disk, dict) else ds_on_disk\n        try:\n         \
          \   # Newer datasets supports native JSON export\n            train_split.to_json(jsonl_path,\
          \ lines=True)\n            logger.info(f\"Wrote JSONL to {jsonl_path} via\
          \ to_json\")\n        except AttributeError:\n            # Manual JSONL\
          \ write\n            import json as _json\n            with open(jsonl_path,\
          \ \"w\") as _f:\n                for _rec in train_split:\n            \
          \        _f.write(_json.dumps(_rec, ensure_ascii=False) + \"\\n\")\n   \
          \         logger.info(f\"Wrote JSONL to {jsonl_path} via manual dump\")\n\
          \    except Exception as _e:\n        logger.warning(f\"Failed to export\
          \ JSONL dataset at {resolved_dataset_dir}: {_e}\")\n        # Leave jsonl_path\
          \ as default; downstream will fallback to directory if file not present\n\
          \n    # ------------------------------\n    # Model resolution (supports\
          \ HF ID/local path or oci:// registry ref)\n    # ------------------------------\n\
          \    def _skopeo_copy_oci_to_layout(oci_ref: str, layout_dir: str) -> None:\n\
          \        \"\"\"Use skopeo to copy a registry image to an OCI layout directory.\"\
          \"\"\n        os.makedirs(layout_dir, exist_ok=True)\n        # Clean previous\
          \ blobs for idempotency if empty or stale\n        try:\n            if\
          \ os.path.isdir(layout_dir) and any(os.scandir(layout_dir)):\n         \
          \       logger.info(f\"OCI layout dir already exists at {layout_dir}\")\n\
          \        except Exception:\n            pass\n        # skopeo syntax: skopeo\
          \ copy docker://REF oci:LAYOUT:TAG\n        # We do not pass auth by default;\
          \ rely on mounted DOCKER_CONFIG/REGISTRY_AUTH_FILE.\n        cmd = [\"skopeo\"\
          , \"copy\", \"-v\", f\"docker://{oci_ref}\", f\"oci:{layout_dir}:latest\"\
          ]\n        logger.info(f\"Running: {' '.join(cmd)}\")\n        res = subprocess.run(cmd,\
          \ text=True, capture_output=True)\n        if res.returncode != 0:\n   \
          \         stderr = (res.stderr or \"\").strip()\n            logger.error(f\"\
          skopeo copy failed (exit={res.returncode}): {stderr}\")\n            if\
          \ \"unauthorized\" in stderr.lower() or \"authentication required\" in stderr.lower():\n\
          \                logger.error(\"Authentication error detected pulling from\
          \ registry. \"\n                             \"Mount a docker config.json\
          \ as DOCKER_CONFIG or provide REGISTRY_AUTH_FILE.\")\n            res.check_returncode()\n\
          \        else:\n            # Stream some of the output to logs for progress\
          \ visibility\n            out_preview = \"\\n\".join((res.stdout or \"\"\
          ).splitlines()[-20:])\n            if out_preview:\n                logger.info(f\"\
          skopeo copy output (tail):\\n{out_preview}\")\n\n    def _olot_extract_models_from_layout(layout_dir:\
          \ str, out_dir: str) -> List[str]:\n        \"\"\"Extract only '/models'\
          \ contents from the OCI layout into out_dir using olot.\"\"\"\n        try:\n\
          \            from olot.basics import crawl_ocilayout_blobs_to_extract\n\
          \        except Exception as e:\n            raise RuntimeError(f\"olot\
          \ is required but failed to import: {e}\")\n        os.makedirs(out_dir,\
          \ exist_ok=True)\n        logger.info(f\"Extracting '/models' from OCI layout\
          \ {layout_dir} to {out_dir}\")\n        extracted = crawl_ocilayout_blobs_to_extract(layout_dir,\
          \ out_dir, tar_filter_dir=\"/models\")\n        logger.info(f\"olot extraction\
          \ complete. Extracted entries: {len(extracted)}\")\n        return extracted\n\
          \n    def _discover_hf_model_dir(root: str) -> _Optional[str]:\n       \
          \ \"\"\"Find a Hugging Face model directory containing config.json, weights,\
          \ and tokenizer.\"\"\"\n        weight_candidates = {\n            \"pytorch_model.bin\"\
          ,\n            \"pytorch_model.bin.index.json\",\n            \"model.safetensors\"\
          ,\n            \"model.safetensors.index.json\",\n        }\n        tokenizer_candidates\
          \ = {\"tokenizer.json\", \"tokenizer.model\"}\n        for dirpath, _dirnames,\
          \ filenames in os.walk(root):\n            fn = set(filenames)\n       \
          \     if \"config.json\" in fn and (fn & weight_candidates) and (fn & tokenizer_candidates):\n\
          \                return dirpath\n        return None\n\n    def _log_dir_tree(root:\
          \ str, max_depth: int = 3, max_entries: int = 800) -> None:\n        \"\"\
          \"Compact tree logger for debugging large directories.\"\"\"\n        try:\n\
          \            if not (root and os.path.isdir(root)):\n                logger.info(f\"\
          (tree) Path is not a directory: {root}\")\n                return\n    \
          \        logger.info(f\"(tree) {root} (max_depth={max_depth}, max_entries={max_entries})\"\
          )\n            total = 0\n            root_depth = root.rstrip(os.sep).count(os.sep)\n\
          \            for dirpath, dirnames, filenames in os.walk(root):\n      \
          \          depth = dirpath.rstrip(os.sep).count(os.sep) - root_depth\n \
          \               if depth >= max_depth:\n                    dirnames[:]\
          \ = []\n                indent = \"  \" * depth\n                logger.info(f\"\
          (tree){indent}{os.path.basename(dirpath) or dirpath}/\")\n             \
          \   total += 1\n                if total >= max_entries:\n             \
          \       logger.info(\"(tree) ... truncated ...\")\n                    return\n\
          \                for fname in sorted(filenames)[:50]:\n                \
          \    logger.info(f\"(tree){indent}  {fname}\")\n                    total\
          \ += 1\n                    if total >= max_entries:\n                 \
          \       logger.info(\"(tree) ... truncated ...\")\n                    \
          \    return\n        except Exception as _e:\n            logger.warning(f\"\
          Failed to render directory tree for {root}: {_e}\")\n\n    resolved_model_path:\
          \ str = training_base_model\n    if isinstance(training_base_model, str)\
          \ and training_base_model.startswith(\"oci://\"):\n        # Strip scheme\
          \ and perform skopeo copy to OCI layout on PVC\n        ref_no_scheme =\
          \ training_base_model[len(\"oci://\") :]\n        layout_dir = os.path.join(pvc_path,\
          \ \"model-oci\")\n        model_out_dir = os.path.join(pvc_path, \"model\"\
          )\n        # Clean output directory for a fresh extraction\n        try:\n\
          \            if os.path.isdir(model_out_dir):\n                shutil.rmtree(model_out_dir)\n\
          \        except Exception:\n            pass\n        _skopeo_copy_oci_to_layout(ref_no_scheme,\
          \ layout_dir)\n        extracted = _olot_extract_models_from_layout(layout_dir,\
          \ model_out_dir)\n        if not extracted:\n            logger.warning(\"\
          No files extracted from '/models' in the OCI artifact; model discovery may\
          \ fail.\")\n        _log_dir_tree(model_out_dir, max_depth=3, max_entries=800)\n\
          \        # Typical extraction path is '<out_dir>/models/...'\n        candidate_root\
          \ = os.path.join(model_out_dir, \"models\")\n        hf_dir = _discover_hf_model_dir(candidate_root\
          \ if os.path.isdir(candidate_root) else model_out_dir)\n        if hf_dir:\n\
          \            logger.info(f\"Detected HuggingFace model directory: {hf_dir}\"\
          )\n            resolved_model_path = hf_dir\n        else:\n           \
          \ logger.warning(\"Failed to detect a HuggingFace model directory after\
          \ extraction; \"\n                           \"continuing with model_out_dir\
          \ (may fail downstream).\")\n            resolved_model_path = model_out_dir\n\
          \n    # ------------------------------\n    # Training (placeholder for\
          \ TrainingHubTrainer)\n    # ------------------------------\n    checkpoints_dir\
          \ = os.path.join(pvc_path, \"checkpoints\")\n    os.makedirs(checkpoints_dir,\
          \ exist_ok=True)\n\n    # Wire in TrainingHubTrainer (modularized steps)\n\
          \    try:\n        from kubeflow.trainer import TrainerClient\n        from\
          \ kubeflow.trainer.rhai import TrainingHubAlgorithms, TrainingHubTrainer\n\
          \        from kubeflow_trainer_api import models as _th_models  # noqa:\
          \ F401\n        from kubeflow.common.types import KubernetesBackendConfig\n\
          \        from kubeflow.trainer.options.kubernetes import (\n           \
          \ PodTemplateOverrides,\n            PodTemplateOverride,\n            PodSpecOverride,\n\
          \            ContainerOverride,\n        )\n\n        if _api_client is\
          \ None:\n            raise RuntimeError(\"Kubernetes API client is not initialized\"\
          )\n\n        backend_cfg = KubernetesBackendConfig(client_configuration=_api_client.configuration)\n\
          \        client = TrainerClient(backend_cfg)\n\n        def _select_runtime(_client)\
          \ -> object:\n            \"\"\"Return the 'training-hub' runtime from Trainer\
          \ backend.\"\"\"\n            for rt in _client.list_runtimes():\n     \
          \           if getattr(rt, \"name\", \"\") == \"training-hub\":\n      \
          \              logger.info(f\"Found runtime: {rt}\")\n                 \
          \   return rt\n            raise RuntimeError(\"Training runtime 'training-hub'\
          \ not found\")\n\n        th_runtime = _select_runtime(client)\n\n     \
          \   # Build training parameters (aligned to OSFT/SFT)\n        parsed_target_patterns\
          \ = [p.strip() for p in training_target_patterns.split(\",\") if p.strip()]\
          \ if training_target_patterns else None\n        parsed_lr_sched_kwargs\
          \ = None\n        if training_lr_scheduler_kwargs:\n            try:\n \
          \               items = [s.strip() for s in training_lr_scheduler_kwargs.split(\"\
          ,\") if s.strip()]\n                kv: Dict[str, str] = {}\n          \
          \      for item in items:\n                    if \"=\" not in item:\n \
          \                       raise ValueError(\n                            f\"\
          Invalid scheduler kwargs segment '{item}'. Expected key=value.\"\n     \
          \                   )\n                    key, value = item.split(\"=\"\
          , 1)\n                    key = key.strip()\n                    value =\
          \ value.strip()\n                    if not key:\n                     \
          \   raise ValueError(\"Empty key in training_lr_scheduler_kwargs\")\n  \
          \                  kv[key] = value\n                parsed_lr_sched_kwargs\
          \ = kv\n            except Exception as e:\n                raise ValueError(f\"\
          Invalid training_lr_scheduler_kwargs format: {e}\")\n\n        def _build_params()\
          \ -> Dict[str, object]:\n            \"\"\"Build OSFT/SFT parameter set\
          \ for TrainingHub.\"\"\"\n            base = {\n                \"model_path\"\
          : resolved_model_path,\n                # Prefer JSONL export when available;\
          \ fallback to resolved directory\n                \"data_path\": jsonl_path\
          \ if os.path.exists(jsonl_path) else resolved_dataset_dir,\n           \
          \     \"effective_batch_size\": int(training_effective_batch_size if training_effective_batch_size\
          \ is not None else 128),\n                \"max_tokens_per_gpu\": int(training_max_tokens_per_gpu),\n\
          \                \"max_seq_len\": int(training_max_seq_len if training_max_seq_len\
          \ is not None else 8192),\n                \"learning_rate\": float(training_learning_rate\
          \ if training_learning_rate is not None else 5e-6),\n                \"\
          backend\": training_backend,\n                \"ckpt_output_dir\": checkpoints_dir,\n\
          \                \"data_output_dir\": training_data_output_dir or os.path.join(checkpoints_dir,\
          \ \"_internal_data_processing\"),\n                \"target_patterns\":\
          \ parsed_target_patterns or [],\n                \"seed\": int(training_seed)\
          \ if training_seed is not None else 42,\n                \"use_liger\":\
          \ bool(training_use_liger) if training_use_liger is not None else False,\n\
          \                \"use_processed_dataset\": bool(training_use_processed_dataset)\
          \ if training_use_processed_dataset is not None else False,\n          \
          \      \"unmask_messages\": bool(training_unmask_messages) if training_unmask_messages\
          \ is not None else False,\n                \"lr_scheduler\": training_lr_scheduler\
          \ or \"constant\",\n                \"warmup_steps\": int(training_lr_warmup_steps)\
          \ if training_lr_warmup_steps is not None else 0,\n                \"save_samples\"\
          : int(training_save_samples) if training_save_samples is not None else 0,\n\
          \                \"accelerate_full_state_at_epoch\": bool(training_accelerate_full_state_at_epoch)\
          \ if training_accelerate_full_state_at_epoch is not None else False,\n \
          \               \"lr_scheduler_kwargs\": parsed_lr_sched_kwargs or {},\n\
          \                \"checkpoint_at_epoch\": bool(training_checkpoint_at_epoch)\
          \ if training_checkpoint_at_epoch is not None else False,\n            \
          \    \"save_final_checkpoint\": bool(training_save_final_checkpoint) if\
          \ training_save_final_checkpoint is not None else False,\n             \
          \   \"num_epochs\": int(training_num_epochs) if training_num_epochs is not\
          \ None else 1,\n            }\n            if (training_algorithm or \"\"\
          ).strip().upper() == \"OSFT\":\n                base[\"unfreeze_rank_ratio\"\
          ] = float(training_unfreeze_rank_ratio)\n            return base\n\n   \
          \     params = _build_params()\n\n        # Algorithm selection: include\
          \ OSFT-only param when applicable\n        algo_value = TrainingHubAlgorithms.OSFT\
          \ if (training_algorithm or \"\").strip().upper() != \"SFT\" else TrainingHubAlgorithms.SFT\n\
          \        if algo_value == TrainingHubAlgorithms.OSFT:\n            params[\"\
          unfreeze_rank_ratio\"] = float(training_unfreeze_rank_ratio)\n\n       \
          \ # Build volumes and mounts (from passthrough only); do not inject env\
          \ via pod overrides\n        # Cluster policy forbids env in podTemplateOverrides;\
          \ use trainer.env for container env\n\n        volumes = []\n        volume_mounts\
          \ = []\n        if kubernetes_config and getattr(kubernetes_config, \"volumes\"\
          , None):\n            volumes.extend(kubernetes_config.volumes)\n      \
          \  if kubernetes_config and getattr(kubernetes_config, \"volume_mounts\"\
          , None):\n            volume_mounts.extend(kubernetes_config.volume_mounts)\n\
          \n        # Container resources are not overridden here; rely on runtime\
          \ defaults or future API support\n\n        # Parse metadata labels/annotations\
          \ for Pod template\n        tpl_labels = parse_kv_list(training_metadata_labels)\n\
          \        tpl_annotations = parse_kv_list(training_metadata_annotations)\n\
          \n        def _build_pod_spec_override() -> PodSpecOverride:\n         \
          \   \"\"\"Return PodSpecOverride with mounts, envs, resources, and scheduling\
          \ hints.\"\"\"\n            return PodSpecOverride(\n                volumes=volumes,\n\
          \                containers=[\n                    ContainerOverride(\n\
          \                        name=\"node\",\n                        volume_mounts=volume_mounts,\n\
          \                    )\n                ],\n                # node_selector=(kubernetes_config.node_selector\
          \ if kubernetes_config and getattr(kubernetes_config, \"node_selector\"\
          , None) else None),\n                # tolerations=(kubernetes_config.tolerations\
          \ if kubernetes_config and getattr(kubernetes_config, \"tolerations\", None)\
          \ else None),\n            )\n\n        job_name = client.train(\n     \
          \       trainer=TrainingHubTrainer(\n                algorithm=TrainingHubAlgorithms.OSFT\
          \ if (training_algorithm or \"\").strip().upper() != \"SFT\" else TrainingHubAlgorithms.SFT,\n\
          \                func_args=params,\n                packages_to_install=[],\n\
          \                # Pass environment variables via Trainer spec (allowed\
          \ by backend/webhook)\n                env=dict(merged_env),\n         \
          \   ),\n            options=[\n                PodTemplateOverrides(\n \
          \                   PodTemplateOverride(\n                        target_jobs=[\"\
          node\"],\n                        metadata={\"labels\": tpl_labels, \"annotations\"\
          : tpl_annotations} if (tpl_labels or tpl_annotations) else None,\n     \
          \                   spec=_build_pod_spec_override(),\n                 \
          \       # numProcsPerWorker=training_resource_num_procs_per_worker,\n  \
          \                      # numWorkers=training_resource_num_workers,\n   \
          \                 )\n                )\n            ],\n            runtime=th_runtime,\n\
          \        )\n        logger.info(f\"Submitted TrainingHub job: {job_name}\"\
          )\n        try:\n            # Wait for the job to start running, then wait\
          \ for completion or failure.\n            client.wait_for_job_status(name=job_name,\
          \ status={\"Running\"}, timeout=300)\n            client.wait_for_job_status(name=job_name,\
          \ status={\"Complete\", \"Failed\"}, timeout=1800)\n            job = client.get_job(name=job_name)\n\
          \            if getattr(job, \"status\", None) == \"Failed\":\n        \
          \        logger.error(\"Training job failed\")\n                raise RuntimeError(f\"\
          Training job failed with status: {job.status}\")\n            elif getattr(job,\
          \ \"status\", None) == \"Complete\":\n                logger.info(\"Training\
          \ job completed successfully\")\n            else:\n                logger.error(f\"\
          Unexpected training job status: {job.status}\")\n                raise RuntimeError(f\"\
          Training job ended with unexpected status: {job.status}\")\n        except\
          \ Exception as e:\n            logger.warning(f\"Training job monitoring\
          \ failed: {e}\")\n    except Exception as e:\n        logger.error(f\"TrainingHubTrainer\
          \ execution failed: {e}\")\n        raise\n\n    # ------------------------------\n\
          \    # Metrics (basic hyperparameters)\n    # ------------------------------\n\
          \    def _log_basic_metrics() -> None:\n        output_metrics.log_metric(\"\
          num_epochs\", float(params.get(\"num_epochs\") or 1))\n        output_metrics.log_metric(\"\
          effective_batch_size\", float(params.get(\"effective_batch_size\") or 128))\n\
          \        output_metrics.log_metric(\"learning_rate\", float(params.get(\"\
          learning_rate\") or 5e-6))\n        output_metrics.log_metric(\"max_seq_len\"\
          , float(params.get(\"max_seq_len\") or 8192))\n        output_metrics.log_metric(\"\
          max_tokens_per_gpu\", float(params.get(\"max_tokens_per_gpu\") or 0))\n\
          \        output_metrics.log_metric(\"unfreeze_rank_ratio\", float(params.get(\"\
          unfreeze_rank_ratio\") or 0))\n\n    _log_basic_metrics()\n\n    # ------------------------------\n\
          \    # Export most recent checkpoint as model artifact (artifact store)\
          \ and PVC\n    # ------------------------------\n    def _persist_and_annotate()\
          \ -> None:\n        \"\"\"Copy latest checkpoint to PVC and artifact store,\
          \ then annotate output metadata.\"\"\"\n        latest = find_model_directory(checkpoints_dir)\n\
          \        if not latest:\n            raise RuntimeError(f\"No model directory\
          \ (with config.json) found under {checkpoints_dir}\")\n        logger.info(f\"\
          Found model directory: {latest}\")\n        # PVC copy\n        pvc_dir\
          \ = os.path.join(pvc_path, \"final_model\")\n        try:\n            if\
          \ os.path.exists(pvc_dir):\n                shutil.rmtree(pvc_dir)\n   \
          \         shutil.copytree(latest, pvc_dir, dirs_exist_ok=True)\n       \
          \     logger.info(f\"Copied checkpoint to PVC dir: {pvc_dir}\")\n      \
          \  except Exception as _e:\n            logger.warning(f\"Failed to copy\
          \ model to PVC dir {pvc_dir}: {_e}\")\n        # Artifact copy\n       \
          \ output_model.name = f\"{training_base_model}-checkpoint\"\n        shutil.copytree(latest,\
          \ output_model.path, dirs_exist_ok=True)\n        logger.info(f\"Exported\
          \ checkpoint from {latest} to artifact path {output_model.path}\")\n   \
          \     # Metadata\n        try:\n            output_model.metadata[\"model_name\"\
          ] = training_base_model\n            output_model.metadata[\"artifact_path\"\
          ] = output_model.path\n            output_model.metadata[\"pvc_model_dir\"\
          ] = pvc_dir\n            logger.info(\"Annotated output_model metadata with\
          \ pvc/artifact locations\")\n        except Exception as _e:\n         \
          \   logger.warning(f\"Failed to set output_model metadata: {_e}\")\n\n \
          \   _persist_and_annotate()\n\n    return \"training completed\"\n\n"
        image: quay.io/opendatahub/odh-training-th03-cuda128-torch28-py312-rhel9@sha256:84d05c5ef9dd3c6ff8173c93dca7e2e6a1cab290f416fb2c469574f89b8e6438
pipelineInfo:
  name: train-model
root:
  dag:
    outputs:
      artifacts:
        output_metrics:
          artifactSelectors:
          - outputArtifactKey: output_metrics
            producerSubtask: train-model
        output_model:
          artifactSelectors:
          - outputArtifactKey: output_model
            producerSubtask: train-model
      parameters:
        Output:
          valueFromParameter:
            outputParameterKey: Output
            producerSubtask: train-model
    tasks:
      train-model:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-train-model
        inputs:
          artifacts:
            dataset:
              componentInputArtifact: dataset
          parameters:
            kubernetes_config:
              componentInputParameter: kubernetes_config
            pvc_path:
              componentInputParameter: pvc_path
            training_accelerate_full_state_at_epoch:
              componentInputParameter: training_accelerate_full_state_at_epoch
            training_algorithm:
              componentInputParameter: training_algorithm
            training_backend:
              componentInputParameter: training_backend
            training_base_model:
              componentInputParameter: training_base_model
            training_checkpoint_at_epoch:
              componentInputParameter: training_checkpoint_at_epoch
            training_data_output_dir:
              componentInputParameter: training_data_output_dir
            training_effective_batch_size:
              componentInputParameter: training_effective_batch_size
            training_envs:
              componentInputParameter: training_envs
            training_hf_token:
              componentInputParameter: training_hf_token
            training_learning_rate:
              componentInputParameter: training_learning_rate
            training_lr_scheduler:
              componentInputParameter: training_lr_scheduler
            training_lr_scheduler_kwargs:
              componentInputParameter: training_lr_scheduler_kwargs
            training_lr_warmup_steps:
              componentInputParameter: training_lr_warmup_steps
            training_max_seq_len:
              componentInputParameter: training_max_seq_len
            training_max_tokens_per_gpu:
              componentInputParameter: training_max_tokens_per_gpu
            training_metadata_annotations:
              componentInputParameter: training_metadata_annotations
            training_metadata_labels:
              componentInputParameter: training_metadata_labels
            training_num_epochs:
              componentInputParameter: training_num_epochs
            training_resource_cpu_per_worker:
              componentInputParameter: training_resource_cpu_per_worker
            training_resource_gpu_per_worker:
              componentInputParameter: training_resource_gpu_per_worker
            training_resource_memory_per_worker:
              componentInputParameter: training_resource_memory_per_worker
            training_resource_num_procs_per_worker:
              componentInputParameter: training_resource_num_procs_per_worker
            training_resource_num_workers:
              componentInputParameter: training_resource_num_workers
            training_save_final_checkpoint:
              componentInputParameter: training_save_final_checkpoint
            training_save_samples:
              componentInputParameter: training_save_samples
            training_seed:
              componentInputParameter: training_seed
            training_target_patterns:
              componentInputParameter: training_target_patterns
            training_unfreeze_rank_ratio:
              componentInputParameter: training_unfreeze_rank_ratio
            training_unmask_messages:
              componentInputParameter: training_unmask_messages
            training_use_liger:
              componentInputParameter: training_use_liger
            training_use_processed_dataset:
              componentInputParameter: training_use_processed_dataset
        taskInfo:
          name: train-model
  inputDefinitions:
    artifacts:
      dataset:
        artifactType:
          schemaTitle: system.Dataset
          schemaVersion: 0.0.1
        description: 'Input dataset artifact (preferred). If not present, this component

          will attempt to load from a remote path specified in dataset.metadata.

          - metadata["artifact_path"]: remote dataset path (e.g., s3://..., https://...,
          or HF repo id)

          - metadata["pvc_dir"]: pre-staged PVC directory to use if present'
        isOptional: true
    parameters:
      kubernetes_config:
        description: TaskConfig passthrough (volumes, mounts, env, resources, tolerations,
          etc.).
        isOptional: true
        parameterType: TASK_CONFIG
      pvc_path:
        description: Root of the workspace PVC for this run.
        parameterType: STRING
      training_accelerate_full_state_at_epoch:
        description: Whether to save full Accelerate state at each epoch (optional).
        isOptional: true
        parameterType: BOOLEAN
      training_algorithm:
        defaultValue: OSFT
        description: Training algorithm ("OSFT" | "SFT"). OSFT adds continual learning
          support.
        isOptional: true
        parameterType: STRING
      training_backend:
        defaultValue: mini-trainer
        description: Trainer backend variant (e.g., "mini-trainer").
        isOptional: true
        parameterType: STRING
      training_base_model:
        defaultValue: Qwen/Qwen2.5-1.5B-Instruct
        description: HuggingFace model ID to fine-tune (e.g., "Qwen/Qwen2.5-1.5B-Instruct").
        isOptional: true
        parameterType: STRING
      training_checkpoint_at_epoch:
        description: Save a checkpoint at each epoch boundary.
        isOptional: true
        parameterType: BOOLEAN
      training_data_output_dir:
        description: Optional secondary output directory on PVC.
        isOptional: true
        parameterType: STRING
      training_effective_batch_size:
        defaultValue: 128.0
        description: "Per-step batch size. Guidance:\n- 1 GPU: 16\u201332\n- 2 GPUs:\
          \ 32\u201364\n- 4 GPUs: 64\u2013128"
        isOptional: true
        parameterType: NUMBER_INTEGER
      training_envs:
        defaultValue: ''
        description: Comma-separated env overrides ("KEY=VAL,KEY=VAL").
        isOptional: true
        parameterType: STRING
      training_hf_token:
        defaultValue: ''
        isOptional: true
        parameterType: STRING
      training_learning_rate:
        description: Learning rate (typ. 1e-6 to 1e-4; 5e-6 is a good OSFT default).
        isOptional: true
        parameterType: NUMBER_DOUBLE
      training_lr_scheduler:
        description: LR scheduler ("cosine" | "linear" | "constant").
        isOptional: true
        parameterType: STRING
      training_lr_scheduler_kwargs:
        defaultValue: ''
        description: 'Comma-delimited key=value string for scheduler kwargs

          (e.g., "num_cycles=1,num_warmup_steps=100").'
        isOptional: true
        parameterType: STRING
      training_lr_warmup_steps:
        description: LR warmup steps (0 for none).
        isOptional: true
        parameterType: NUMBER_INTEGER
      training_max_seq_len:
        defaultValue: 8192.0
        description: "Max sequence length (typical 2048\u20138192)."
        isOptional: true
        parameterType: NUMBER_INTEGER
      training_max_tokens_per_gpu:
        defaultValue: 64000.0
        description: Token budget per GPU for memory mgmt.
        isOptional: true
        parameterType: NUMBER_INTEGER
      training_metadata_annotations:
        defaultValue: ''
        description: Comma-separated annotations ("k=v,k=v") for pod template.
        isOptional: true
        parameterType: STRING
      training_metadata_labels:
        defaultValue: ''
        description: Comma-separated labels ("k=v,k=v") for pod template.
        isOptional: true
        parameterType: STRING
      training_num_epochs:
        description: "Number of epochs (1 = quick test; 3\u20135 = better convergence)."
        isOptional: true
        parameterType: NUMBER_INTEGER
      training_resource_cpu_per_worker:
        defaultValue: '8'
        description: CPU limit/request per worker (e.g., "8").
        isOptional: true
        parameterType: STRING
      training_resource_gpu_per_worker:
        defaultValue: 1.0
        description: GPUs per worker (e.g., 1). Typically equals num procs.
        isOptional: true
        parameterType: NUMBER_INTEGER
      training_resource_memory_per_worker:
        defaultValue: 32Gi
        description: Memory per worker (e.g., "32Gi").
        isOptional: true
        parameterType: STRING
      training_resource_num_procs_per_worker:
        defaultValue: 1.0
        description: Processes (ranks) per worker (usually equals GPUs/worker).
        isOptional: true
        parameterType: NUMBER_INTEGER
      training_resource_num_workers:
        defaultValue: 1.0
        description: Total worker pods (1 = single-node; 2+ = multi-node).
        isOptional: true
        parameterType: NUMBER_INTEGER
      training_save_final_checkpoint:
        description: Save the final model checkpoint.
        isOptional: true
        parameterType: BOOLEAN
      training_save_samples:
        description: Number of samples to save during SFT (optional).
        isOptional: true
        parameterType: NUMBER_INTEGER
      training_seed:
        description: Random seed for reproducibility.
        isOptional: true
        parameterType: NUMBER_INTEGER
      training_target_patterns:
        defaultValue: ''
        description: Comma-separated target modules/patterns (algorithm-specific).
        isOptional: true
        parameterType: STRING
      training_unfreeze_rank_ratio:
        defaultValue: 0.25
        isOptional: true
        parameterType: NUMBER_DOUBLE
      training_unmask_messages:
        description: Whether to unmask chat messages if applicable.
        isOptional: true
        parameterType: BOOLEAN
      training_use_liger:
        description: Enable Liger kernel optimizations (image must include kernels).
        isOptional: true
        parameterType: BOOLEAN
      training_use_processed_dataset:
        description: Whether dataset is already processed.
        isOptional: true
        parameterType: BOOLEAN
  outputDefinitions:
    artifacts:
      output_metrics:
        artifactType:
          schemaTitle: system.Metrics
          schemaVersion: 0.0.1
      output_model:
        artifactType:
          schemaTitle: system.Model
          schemaVersion: 0.0.1
    parameters:
      Output:
        parameterType: STRING
  taskConfigPassthroughs:
  - field: RESOURCES
  - field: KUBERNETES_TOLERATIONS
  - field: KUBERNETES_NODE_SELECTOR
  - field: KUBERNETES_AFFINITY
  - applyToTask: true
    field: ENV
  - applyToTask: true
    field: KUBERNETES_VOLUMES
schemaVersion: 2.1.0
sdkVersion: kfp-2.15.2
